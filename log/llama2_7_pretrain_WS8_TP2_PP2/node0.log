using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: overriding default arguments for tokenizer_type:LLaMaSentencePieceTokenizer                        with tokenizer_type:Llama2Tokenizer
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
WARNING: Setting args.check_for_nan_in_loss_and_grad to False since dynamic loss scaling is being used
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... True
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. False
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_fully_parallel_save ........................ False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  data_cache_path ................................. ./data_cache/llama2_7_pretrain_WS8_TP2_PP2
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  do_trace ........................................ True
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... False
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 512
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 0
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 100
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  fake_dp ......................................... 4
  fake_gpus_per_node .............................. 8
  fake_local_rank ................................. 0
  fake_pp ......................................... 8
  fake_tp ......................................... 8
  fake_world_size ................................. 256
  fake_wrank ...................................... 0
  ffn_hidden_size ................................. 11008
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 2
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  is_scaling_mode ................................. True
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 3e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.1
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  main_tokenizer_type ............................. LLaMaSentencePieceTokenizer
  make_vocab_size_divisible_by .................... 1
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 512
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-06
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_dropping .............................. False
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  nsight_start .................................... 3
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 32
  num_workers ..................................... 0
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1403
  seq_length ...................................... 512
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /research/d1/gds/ytyang/yichengfeng/Megatron-LM/tokenizers/Llama2Tokenizer/tokenizer.model
  tokenizer_type .................................. Llama2Tokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  trace_start ..................................... 3
  train_data_path ................................. None
  train_iters ..................................... 3
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_mcore_models ................................ True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... 3200
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 1
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 2
> building Llama2Tokenizer tokenizer ...
 > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1403 ...
> compiling dataset index builder ...
make: Entering directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.048 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 0.875 seconds
/research/d1/gds/ytyang/yichengfeng/Megatron-LM/megatron/training/initialize.py:405: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400412039/work/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
  output = bias_gelu(bias, input)
time to initialize megatron (seconds): 2.136
[after megatron is initialized] datetime: 2024-12-06 15:24:25 
mpu_info:MPUInfo:
	dp_size=4
	tp_size=8
	pp_size=8
	mp_size=64
	world_size=256
	dp_groups=[[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27], [4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31], [32, 40, 48, 56], [33, 41, 49, 57], [34, 42, 50, 58], [35, 43, 51, 59], [36, 44, 52, 60], [37, 45, 53, 61], [38, 46, 54, 62], [39, 47, 55, 63], [64, 72, 80, 88], [65, 73, 81, 89], [66, 74, 82, 90], [67, 75, 83, 91], [68, 76, 84, 92], [69, 77, 85, 93], [70, 78, 86, 94], [71, 79, 87, 95], [96, 104, 112, 120], [97, 105, 113, 121], [98, 106, 114, 122], [99, 107, 115, 123], [100, 108, 116, 124], [101, 109, 117, 125], [102, 110, 118, 126], [103, 111, 119, 127], [128, 136, 144, 152], [129, 137, 145, 153], [130, 138, 146, 154], [131, 139, 147, 155], [132, 140, 148, 156], [133, 141, 149, 157], [134, 142, 150, 158], [135, 143, 151, 159], [160, 168, 176, 184], [161, 169, 177, 185], [162, 170, 178, 186], [163, 171, 179, 187], [164, 172, 180, 188], [165, 173, 181, 189], [166, 174, 182, 190], [167, 175, 183, 191], [192, 200, 208, 216], [193, 201, 209, 217], [194, 202, 210, 218], [195, 203, 211, 219], [196, 204, 212, 220], [197, 205, 213, 221], [198, 206, 214, 222], [199, 207, 215, 223], [224, 232, 240, 248], [225, 233, 241, 249], [226, 234, 242, 250], [227, 235, 243, 251], [228, 236, 244, 252], [229, 237, 245, 253], [230, 238, 246, 254], [231, 239, 247, 255]]
	pp_groups=[[0, 32, 64, 96, 128, 160, 192, 224], [1, 33, 65, 97, 129, 161, 193, 225], [2, 34, 66, 98, 130, 162, 194, 226], [3, 35, 67, 99, 131, 163, 195, 227], [4, 36, 68, 100, 132, 164, 196, 228], [5, 37, 69, 101, 133, 165, 197, 229], [6, 38, 70, 102, 134, 166, 198, 230], [7, 39, 71, 103, 135, 167, 199, 231], [8, 40, 72, 104, 136, 168, 200, 232], [9, 41, 73, 105, 137, 169, 201, 233], [10, 42, 74, 106, 138, 170, 202, 234], [11, 43, 75, 107, 139, 171, 203, 235], [12, 44, 76, 108, 140, 172, 204, 236], [13, 45, 77, 109, 141, 173, 205, 237], [14, 46, 78, 110, 142, 174, 206, 238], [15, 47, 79, 111, 143, 175, 207, 239], [16, 48, 80, 112, 144, 176, 208, 240], [17, 49, 81, 113, 145, 177, 209, 241], [18, 50, 82, 114, 146, 178, 210, 242], [19, 51, 83, 115, 147, 179, 211, 243], [20, 52, 84, 116, 148, 180, 212, 244], [21, 53, 85, 117, 149, 181, 213, 245], [22, 54, 86, 118, 150, 182, 214, 246], [23, 55, 87, 119, 151, 183, 215, 247], [24, 56, 88, 120, 152, 184, 216, 248], [25, 57, 89, 121, 153, 185, 217, 249], [26, 58, 90, 122, 154, 186, 218, 250], [27, 59, 91, 123, 155, 187, 219, 251], [28, 60, 92, 124, 156, 188, 220, 252], [29, 61, 93, 125, 157, 189, 221, 253], [30, 62, 94, 126, 158, 190, 222, 254], [31, 63, 95, 127, 159, 191, 223, 255]]
	tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47], [48, 49, 50, 51, 52, 53, 54, 55], [56, 57, 58, 59, 60, 61, 62, 63], [64, 65, 66, 67, 68, 69, 70, 71], [72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87], [88, 89, 90, 91, 92, 93, 94, 95], [96, 97, 98, 99, 100, 101, 102, 103], [104, 105, 106, 107, 108, 109, 110, 111], [112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127], [128, 129, 130, 131, 132, 133, 134, 135], [136, 137, 138, 139, 140, 141, 142, 143], [144, 145, 146, 147, 148, 149, 150, 151], [152, 153, 154, 155, 156, 157, 158, 159], [160, 161, 162, 163, 164, 165, 166, 167], [168, 169, 170, 171, 172, 173, 174, 175], [176, 177, 178, 179, 180, 181, 182, 183], [184, 185, 186, 187, 188, 189, 190, 191], [192, 193, 194, 195, 196, 197, 198, 199], [200, 201, 202, 203, 204, 205, 206, 207], [208, 209, 210, 211, 212, 213, 214, 215], [216, 217, 218, 219, 220, 221, 222, 223], [224, 225, 226, 227, 228, 229, 230, 231], [232, 233, 234, 235, 236, 237, 238, 239], [240, 241, 242, 243, 244, 245, 246, 247], [248, 249, 250, 251, 252, 253, 254, 255]]
	mp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67, 68, 69, 70, 71, 96, 97, 98, 99, 100, 101, 102, 103, 128, 129, 130, 131, 132, 133, 134, 135, 160, 161, 162, 163, 164, 165, 166, 167, 192, 193, 194, 195, 196, 197, 198, 199, 224, 225, 226, 227, 228, 229, 230, 231], [8, 9, 10, 11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47, 72, 73, 74, 75, 76, 77, 78, 79, 104, 105, 106, 107, 108, 109, 110, 111, 136, 137, 138, 139, 140, 141, 142, 143, 168, 169, 170, 171, 172, 173, 174, 175, 200, 201, 202, 203, 204, 205, 206, 207, 232, 233, 234, 235, 236, 237, 238, 239], [16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55, 80, 81, 82, 83, 84, 85, 86, 87, 112, 113, 114, 115, 116, 117, 118, 119, 144, 145, 146, 147, 148, 149, 150, 151, 176, 177, 178, 179, 180, 181, 182, 183, 208, 209, 210, 211, 212, 213, 214, 215, 240, 241, 242, 243, 244, 245, 246, 247], [24, 25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63, 88, 89, 90, 91, 92, 93, 94, 95, 120, 121, 122, 123, 124, 125, 126, 127, 152, 153, 154, 155, 156, 157, 158, 159, 184, 185, 186, 187, 188, 189, 190, 191, 216, 217, 218, 219, 220, 221, 222, 223, 248, 249, 250, 251, 252, 253, 254, 255]]
	ep_groups=[[0, 224], [1, 225], [2, 226], [3, 227], [4, 228], [5, 229], [6, 230], [7, 231], [8, 232], [9, 233], [10, 234], [11, 235], [12, 236], [13, 237], [14, 238], [15, 239], [16, 240], [17, 241], [18, 242], [19, 243], [20, 244], [21, 245], [22, 246], [23, 247], [24, 248], [25, 249], [26, 250], [27, 251], [28, 252], [29, 253], [30, 254], [31, 255]]
	pep_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 0, finish warm up ...
rank_id = 0, input_tensor_shapes: []
rank:0,cuda fwd time: 9.138463973999023
rank:0, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.03,timestamp=2425172283.98,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172285.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172286.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172288.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172288.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172289.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172290.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172291.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172292.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 0, finish FWD profile ...
rank:0, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425172293.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172294.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425172295.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425172296.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425172297.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172298.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425172299.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172300.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:0, finish BWD profile ...
rank:0,optimizer_step time: 2.161695957183838
rank:0, finish optimizer.step profile ...
rank:0, Before memory release - Allocated: 2389533696, Reserved: 2478833664
rank:0, trace log has been written to txt...
rank:0, finish release GPU memory ...
rank:0, After memory release - Allocated: 477697024, Reserved: 494927872
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 1, finish warm up ...
rank_id = 1, input_tensor_shapes: []
rank:1,cuda fwd time: 7.709695816040039
rank:1, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425172534.58,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172535.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172536.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172537.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172538.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172539.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172540.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172541.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172541.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 1, finish FWD profile ...
rank:1, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.03,timestamp=2425172543.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.04,timestamp=2425172544.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172545.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425172546.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425172548.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425172549.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425172550.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425172551.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:1, finish BWD profile ...
rank:1,optimizer_step time: 14.38003158569336
rank:1, finish optimizer.step profile ...
rank:1, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:1, trace log has been written to txt...
rank:1, finish release GPU memory ...
rank:1, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 2, finish warm up ...
rank_id = 2, input_tensor_shapes: []
rank:2,cuda fwd time: 7.797760009765625
rank:2, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425172756.64,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172757.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172758.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172759.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172760.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172761.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172762.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172763.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172763.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 2, finish FWD profile ...
rank:2, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.1,timestamp=2425172766.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172769.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425172771.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172774.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172776.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172779.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172781.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172784.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:2, finish BWD profile ...
rank:2,optimizer_step time: 1.7817599773406982
rank:2, finish optimizer.step profile ...
rank:2, Before memory release - Allocated: 1908691968, Reserved: 2042626048
rank:2, trace log has been written to txt...
rank:2, finish release GPU memory ...
rank:2, After memory release - Allocated: 477697024, Reserved: 1203765248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 3, finish warm up ...
rank_id = 3, input_tensor_shapes: []
rank:3,cuda fwd time: 7.7506561279296875
rank:3, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425172977.62,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172978.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172979.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172980.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172981.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172982.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172983.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172984.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425172984.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 3, finish FWD profile ...
rank:3, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425172987.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172990.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172992.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172995.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425172997.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173000.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173002.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173005.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:3, finish BWD profile ...
rank:3,optimizer_step time: 1.7868800163269043
rank:3, finish optimizer.step profile ...
rank:3, Before memory release - Allocated: 1908691968, Reserved: 2042626048
rank:3, trace log has been written to txt...
rank:3, finish release GPU memory ...
rank:3, After memory release - Allocated: 477697024, Reserved: 1203765248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 4, finish warm up ...
rank_id = 4, input_tensor_shapes: []
rank:4,cuda fwd time: 9.359359741210938
rank:4, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425173195.13,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173196.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173197.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173198.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173199.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173200.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173201.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173203.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173203.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 4, finish FWD profile ...
rank:4, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425173206.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173209.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425173211.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173214.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173216.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173219.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425173221.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173224.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:4, finish BWD profile ...
rank:4,optimizer_step time: 1.7879040241241455
rank:4, finish optimizer.step profile ...
rank:4, Before memory release - Allocated: 1908691968, Reserved: 2042626048
rank:4, trace log has been written to txt...
rank:4, finish release GPU memory ...
rank:4, After memory release - Allocated: 477697024, Reserved: 1203765248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 5, finish warm up ...
rank_id = 5, input_tensor_shapes: []
rank:5,cuda fwd time: 9.1146240234375
rank:5, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425173406.91,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173408.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173409.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173410.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173411.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173412.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173413.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173414.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173415.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 5, finish FWD profile ...
rank:5, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425173418.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173420.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173423.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173426.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173428.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173431.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425173433.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173436.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:5, finish BWD profile ...
rank:5,optimizer_step time: 1.785856008529663
rank:5, finish optimizer.step profile ...
rank:5, Before memory release - Allocated: 1908691968, Reserved: 2042626048
rank:5, trace log has been written to txt...
rank:5, finish release GPU memory ...
rank:5, After memory release - Allocated: 477697024, Reserved: 1203765248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 6, finish warm up ...
rank_id = 6, input_tensor_shapes: []
rank:6,cuda fwd time: 7.772160053253174
rank:6, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425173603.42,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173604.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173605.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173606.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173607.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173608.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173608.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173609.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173610.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 6, finish FWD profile ...
rank:6, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425173613.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173616.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173618.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173621.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173623.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173626.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173628.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173631.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:6, finish BWD profile ...
rank:6,optimizer_step time: 1.7715200185775757
rank:6, finish optimizer.step profile ...
rank:6, Before memory release - Allocated: 1907643392, Reserved: 2034237440
rank:6, trace log has been written to txt...
rank:6, finish release GPU memory ...
rank:6, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 117654272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 7, finish warm up ...
rank_id = 7, input_tensor_shapes: []
rank:7,cuda fwd time: 7.734272003173828
rank:7, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425173820.16,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173821.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173822.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173823.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173823.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173824.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173825.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173826.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425173827.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 7, finish FWD profile ...
rank:7, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425173829.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173832.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173835.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173837.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173840.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173842.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173845.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425173847.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:7, finish BWD profile ...
rank:7,optimizer_step time: 1.7940479516983032
rank:7, finish optimizer.step profile ...
rank:7, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:7, trace log has been written to txt...
rank:7, finish release GPU memory ...
rank:7, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 8, finish warm up ...
rank_id = 8, input_tensor_shapes: []
rank:8,cuda fwd time: 9.138303756713867
rank:8, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425174038.08,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174039.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174040.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174041.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174042.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174043.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174044.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174045.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174046.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 8, finish FWD profile ...
rank:8, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425174047.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174048.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425174049.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174050.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425174051.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174052.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425174053.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174054.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:8, finish BWD profile ...
rank:8,optimizer_step time: 1.7388800382614136
rank:8, finish optimizer.step profile ...
rank:8, Before memory release - Allocated: 2863035392, Reserved: 2971664384
rank:8, trace log has been written to txt...
rank:8, finish release GPU memory ...
rank:8, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 9, finish warm up ...
rank_id = 9, input_tensor_shapes: []
rank:9,cuda fwd time: 7.775231838226318
rank:9, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425174339.66,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174340.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174341.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174342.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174343.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174344.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174345.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174346.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174346.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 9, finish FWD profile ...
rank:9, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425174349.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174352.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174354.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174357.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174359.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174362.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174364.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174367.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:9, finish BWD profile ...
rank:9,optimizer_step time: 1.7786879539489746
rank:9, finish optimizer.step profile ...
rank:9, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:9, trace log has been written to txt...
rank:9, finish release GPU memory ...
rank:9, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 10, finish warm up ...
rank_id = 10, input_tensor_shapes: []
rank:10,cuda fwd time: 10.605567932128906
rank:10, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.03,timestamp=2425174560.88,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174562.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425174563.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174564.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425174565.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174567.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174568.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174569.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174570.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 10, finish FWD profile ...
rank:10, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425174573.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174576.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174578.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174581.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174583.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174586.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174588.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174591.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:10, finish BWD profile ...
rank:10,optimizer_step time: 1.814527988433838
rank:10, finish optimizer.step profile ...
rank:10, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:10, trace log has been written to txt...
rank:10, finish release GPU memory ...
rank:10, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 11, finish warm up ...
rank_id = 11, input_tensor_shapes: []
rank:11,cuda fwd time: 7.784448146820068
rank:11, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425174764.44,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174765.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174766.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174767.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174768.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174769.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174769.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174771.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174771.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 11, finish FWD profile ...
rank:11, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.12,timestamp=2425174774.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174777.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174779.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174782.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174784.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174787.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174789.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425174792.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:11, finish BWD profile ...
rank:11,optimizer_step time: 1.7868800163269043
rank:11, finish optimizer.step profile ...
rank:11, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:11, trace log has been written to txt...
rank:11, finish release GPU memory ...
rank:11, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 12, finish warm up ...
rank_id = 12, input_tensor_shapes: []
rank:12,cuda fwd time: 10.044416427612305
rank:12, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425174988.68,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174990.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174991.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174992.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174993.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174994.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174995.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174997.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425174998.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 12, finish FWD profile ...
rank:12, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425175000.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175003.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175005.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175008.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175010.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175013.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175016.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175018.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:12, finish BWD profile ...
rank:12,optimizer_step time: 1.8001919984817505
rank:12, finish optimizer.step profile ...
rank:12, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:12, trace log has been written to txt...
rank:12, finish release GPU memory ...
rank:12, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 13, finish warm up ...
rank_id = 13, input_tensor_shapes: []
rank:13,cuda fwd time: 8.349696159362793
rank:13, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425175185.49,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175186.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175187.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175188.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175189.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175190.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175191.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175192.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175193.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 13, finish FWD profile ...
rank:13, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425175195.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175198.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175201.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175203.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175206.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175208.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175211.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175213.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:13, finish BWD profile ...
rank:13,optimizer_step time: 1.773568034172058
rank:13, finish optimizer.step profile ...
rank:13, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:13, trace log has been written to txt...
rank:13, finish release GPU memory ...
rank:13, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 14, finish warm up ...
rank_id = 14, input_tensor_shapes: []
rank:14,cuda fwd time: 8.338432312011719
rank:14, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425175382.26,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175383.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175384.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175385.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175386.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175387.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175388.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175389.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175390.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 14, finish FWD profile ...
rank:14, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425175392.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175395.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175397.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175400.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175402.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175405.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175407.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175410.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:14, finish BWD profile ...
rank:14,optimizer_step time: 1.7838079929351807
rank:14, finish optimizer.step profile ...
rank:14, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:14, trace log has been written to txt...
rank:14, finish release GPU memory ...
rank:14, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 15, finish warm up ...
rank_id = 15, input_tensor_shapes: []
rank:15,cuda fwd time: 7.64518404006958
rank:15, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425175589.78,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175590.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175591.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175592.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175593.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175594.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175595.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175596.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175596.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 15, finish FWD profile ...
rank:15, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425175599.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425175602.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175604.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175607.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175609.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175612.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175614.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425175617.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:15, finish BWD profile ...
rank:15,optimizer_step time: 1.780735969543457
rank:15, finish optimizer.step profile ...
rank:15, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:15, trace log has been written to txt...
rank:15, finish release GPU memory ...
rank:15, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 16, finish warm up ...
rank_id = 16, input_tensor_shapes: []
rank:16,cuda fwd time: 9.72764778137207
rank:16, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.07,timestamp=2425175806.18,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175808.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175809.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175810.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175811.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175812.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175813.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175814.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175815.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 16, finish FWD profile ...
rank:16, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425175816.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175817.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425175818.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175819.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425175819.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175820.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425175821.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425175822.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:16, finish BWD profile ...
rank:16,optimizer_step time: 1.737663984298706
rank:16, finish optimizer.step profile ...
rank:16, Before memory release - Allocated: 2863035392, Reserved: 2971664384
rank:16, trace log has been written to txt...
rank:16, finish release GPU memory ...
rank:16, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 17, finish warm up ...
rank_id = 17, input_tensor_shapes: []
rank:17,cuda fwd time: 7.615488052368164
rank:17, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425176002.88,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176004.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176004.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176006.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176006.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176007.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176008.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176009.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176010.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 17, finish FWD profile ...
rank:17, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425176011.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176013.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.04,timestamp=2425176014.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.04,timestamp=2425176016.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.04,timestamp=2425176018.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176020.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.04,timestamp=2425176022.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.04,timestamp=2425176024.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:17, finish BWD profile ...
rank:17,optimizer_step time: 1.7500159740447998
rank:17, finish optimizer.step profile ...
rank:17, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:17, trace log has been written to txt...
rank:17, finish release GPU memory ...
rank:17, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 18, finish warm up ...
rank_id = 18, input_tensor_shapes: []
rank:18,cuda fwd time: 8.359935760498047
rank:18, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425176213.25,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176214.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176215.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176216.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176217.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176218.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176219.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176220.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176221.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 18, finish FWD profile ...
rank:18, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425176223.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176225.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176228.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176230.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176232.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176234.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176237.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425176239.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:18, finish BWD profile ...
rank:18,optimizer_step time: 1.7776639461517334
rank:18, finish optimizer.step profile ...
rank:18, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:18, trace log has been written to txt...
rank:18, finish release GPU memory ...
rank:18, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 19, finish warm up ...
rank_id = 19, input_tensor_shapes: []
rank:19,cuda fwd time: 7.408639907836914
rank:19, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425176632.26,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176633.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176634.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176635.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176635.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176636.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176637.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176638.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176639.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 19, finish FWD profile ...
rank:19, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425176641.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176644.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176646.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176649.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176651.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176654.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176656.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425176659.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:19, finish BWD profile ...
rank:19,optimizer_step time: 1.798143982887268
rank:19, finish optimizer.step profile ...
rank:19, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:19, trace log has been written to txt...
rank:19, finish release GPU memory ...
rank:19, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 20, finish warm up ...
rank_id = 20, input_tensor_shapes: []
rank:20,cuda fwd time: 6.760447978973389
rank:20, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425176846.47,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176847.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176848.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176849.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176849.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176850.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176851.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425176852.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425176852.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 20, finish FWD profile ...
rank:20, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425176855.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176858.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176860.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176863.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176865.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176868.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176870.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425176873.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:20, finish BWD profile ...
rank:20,optimizer_step time: 1.7756160497665405
rank:20, finish optimizer.step profile ...
rank:20, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:20, trace log has been written to txt...
rank:20, finish release GPU memory ...
rank:20, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 21, finish warm up ...
rank_id = 21, input_tensor_shapes: []
rank:21,cuda fwd time: 6.585343837738037
rank:21, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425177035.76,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177036.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425177037.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425177038.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425177038.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177039.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425177040.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425177041.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425177041.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 21, finish FWD profile ...
rank:21, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425177044.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177047.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177049.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177052.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177054.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177057.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177059.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177062.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:21, finish BWD profile ...
rank:21,optimizer_step time: 1.7479679584503174
rank:21, finish optimizer.step profile ...
rank:21, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:21, trace log has been written to txt...
rank:21, finish release GPU memory ...
rank:21, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 22, finish warm up ...
rank_id = 22, input_tensor_shapes: []
rank:22,cuda fwd time: 8.08140754699707
rank:22, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425177237.13,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177238.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177239.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177240.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177241.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177242.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177242.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177243.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177244.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 22, finish FWD profile ...
rank:22, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425177247.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.1,timestamp=2425177250.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177252.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177255.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177257.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177260.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177262.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177265.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:22, finish BWD profile ...
rank:22,optimizer_step time: 1.772544026374817
rank:22, finish optimizer.step profile ...
rank:22, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:22, trace log has been written to txt...
rank:22, finish release GPU memory ...
rank:22, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 23, finish warm up ...
rank_id = 23, input_tensor_shapes: []
rank:23,cuda fwd time: 8.325119972229004
rank:23, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425177437.0,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177438.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177439.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177440.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177441.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177442.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177442.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177444.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177444.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 23, finish FWD profile ...
rank:23, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425177447.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177450.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177452.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177455.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177457.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177460.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177462.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425177465.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:23, finish BWD profile ...
rank:23,optimizer_step time: 1.773568034172058
rank:23, finish optimizer.step profile ...
rank:23, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:23, trace log has been written to txt...
rank:23, finish release GPU memory ...
rank:23, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 24, finish warm up ...
rank_id = 24, input_tensor_shapes: []
rank:24,cuda fwd time: 8.17024040222168
rank:24, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425177644.15,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177645.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177646.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177647.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177648.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177649.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177649.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177651.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177651.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 24, finish FWD profile ...
rank:24, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425177652.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177653.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425177654.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177655.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425177656.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177657.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425177657.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177658.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:24, finish BWD profile ...
rank:24,optimizer_step time: 1.7261120080947876
rank:24, finish optimizer.step profile ...
rank:24, Before memory release - Allocated: 2863035392, Reserved: 2971664384
rank:24, trace log has been written to txt...
rank:24, finish release GPU memory ...
rank:24, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 25, finish warm up ...
rank_id = 25, input_tensor_shapes: []
rank:25,cuda fwd time: 8.048640251159668
rank:25, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425177834.37,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177835.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177836.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177837.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177838.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177839.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177840.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177841.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177841.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 25, finish FWD profile ...
rank:25, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425177843.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177843.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425177844.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425177845.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425177846.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425177848.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425177850.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425177852.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:25, finish BWD profile ...
rank:25,optimizer_step time: 1.7469439506530762
rank:25, finish optimizer.step profile ...
rank:25, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:25, trace log has been written to txt...
rank:25, finish release GPU memory ...
rank:25, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 26, finish warm up ...
rank_id = 26, input_tensor_shapes: []
rank:26,cuda fwd time: 6.659071922302246
rank:26, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425178007.99,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178009.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178009.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178010.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178011.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178012.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178012.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178013.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178014.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 26, finish FWD profile ...
rank:26, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425178016.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178019.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425178021.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425178023.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425178025.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425178027.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425178029.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425178032.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:26, finish BWD profile ...
rank:26,optimizer_step time: 1.7059839963912964
rank:26, finish optimizer.step profile ...
rank:26, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:26, trace log has been written to txt...
rank:26, finish release GPU memory ...
rank:26, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 27, finish warm up ...
rank_id = 27, input_tensor_shapes: []
rank:27,cuda fwd time: 8.045568466186523
rank:27, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425178208.51,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178209.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178210.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178211.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178212.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178213.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178214.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178215.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178216.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 27, finish FWD profile ...
rank:27, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425178218.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178221.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425178223.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178226.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178228.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178231.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178233.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178236.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:27, finish BWD profile ...
rank:27,optimizer_step time: 1.716223955154419
rank:27, finish optimizer.step profile ...
rank:27, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:27, trace log has been written to txt...
rank:27, finish release GPU memory ...
rank:27, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 28, finish warm up ...
rank_id = 28, input_tensor_shapes: []
rank:28,cuda fwd time: 8.085503578186035
rank:28, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425178408.92,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178410.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178410.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178412.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178412.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178413.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178414.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178415.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178416.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 28, finish FWD profile ...
rank:28, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425178418.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178421.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178424.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178426.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178429.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178431.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178434.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178436.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:28, finish BWD profile ...
rank:28,optimizer_step time: 1.7203199863433838
rank:28, finish optimizer.step profile ...
rank:28, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:28, trace log has been written to txt...
rank:28, finish release GPU memory ...
rank:28, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 29, finish warm up ...
rank_id = 29, input_tensor_shapes: []
rank:29,cuda fwd time: 8.615936279296875
rank:29, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425178616.24,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178617.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178618.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178619.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178620.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178621.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178622.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178623.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178624.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 29, finish FWD profile ...
rank:29, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425178626.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178629.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178632.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178634.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425178637.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178639.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178642.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178644.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:29, finish BWD profile ...
rank:29,optimizer_step time: 1.7254400253295898
rank:29, finish optimizer.step profile ...
rank:29, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:29, trace log has been written to txt...
rank:29, finish release GPU memory ...
rank:29, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 30, finish warm up ...
rank_id = 30, input_tensor_shapes: []
rank:30,cuda fwd time: 6.599679946899414
rank:30, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425178811.12,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425178812.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178812.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178813.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178814.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178815.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178815.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178816.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425178817.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 30, finish FWD profile ...
rank:30, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.08,timestamp=2425178819.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178822.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178824.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425178827.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178829.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178832.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178834.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425178837.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:30, finish BWD profile ...
rank:30,optimizer_step time: 1.7233920097351074
rank:30, finish optimizer.step profile ...
rank:30, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:30, trace log has been written to txt...
rank:30, finish release GPU memory ...
rank:30, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117654272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.embedding.word_embeddings.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 31, finish warm up ...
rank_id = 31, input_tensor_shapes: []
rank:31,cuda fwd time: 6.748159885406494
rank:31, fwd_subop num: 9, fwd_subop: ['trace_src_func=_reduce,duration=0.02,timestamp=2425179008.52,input__shape=[1, 512, 4096],input__dtype=torch.float16,func_name=embedding_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179009.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179010.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179011.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179011.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179012.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179013.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179014.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179014.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 31, finish FWD profile ...
rank:31, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425179017.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179020.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179022.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179025.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179027.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179030.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179032.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179035.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:31, finish BWD profile ...
rank:31,optimizer_step time: 1.7244160175323486
rank:31, finish optimizer.step profile ...
rank:31, Before memory release - Allocated: 1909216256, Reserved: 2038431744
rank:31, trace log has been written to txt...
rank:31, finish release GPU memory ...
rank:31, After memory release - Allocated: 477697024, Reserved: 966787072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 32, finish warm up ...
rank_id = 32, input_tensor_shapes: [(512, 1, 4096)]
rank:32,cuda fwd time: 7.106560230255127
rank:32, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425179240.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179241.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179242.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179242.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179243.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179244.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179245.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179246.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 32, finish FWD profile ...
rank:32, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425179248.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425179251.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179254.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179256.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179259.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179261.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425179263.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179266.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:32, finish BWD profile ...
rank:32,optimizer_step time: 5.208064079284668
rank:32, finish optimizer.step profile ...
rank:32, Before memory release - Allocated: 2561306112, Reserved: 2657091584
rank:32, trace log has been written to txt...
rank:32, finish release GPU memory ...
rank:32, After memory release - Allocated: 910042112, Reserved: 996147200
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 33, finish warm up ...
rank_id = 33, input_tensor_shapes: [(512, 1, 4096)]
rank:33,cuda fwd time: 6.740992069244385
rank:33, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425179459.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179460.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179461.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179461.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179462.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179463.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179464.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179464.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 33, finish FWD profile ...
rank:33, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425179467.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179470.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179472.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179475.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179477.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179480.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425179482.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179485.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:33, finish BWD profile ...
rank:33,optimizer_step time: 4.802559852600098
rank:33, finish optimizer.step profile ...
rank:33, Before memory release - Allocated: 2993747968, Reserved: 3093299200
rank:33, trace log has been written to txt...
rank:33, finish release GPU memory ...
rank:33, After memory release - Allocated: 911022080, Reserved: 1407188992
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 34, finish warm up ...
rank_id = 34, input_tensor_shapes: [(512, 1, 4096)]
rank:34,cuda fwd time: 6.329343795776367
rank:34, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425179669.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179669.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179670.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179671.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179672.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179672.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179673.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179674.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 34, finish FWD profile ...
rank:34, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425179676.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179679.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179681.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179684.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179686.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179689.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425179691.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179694.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:34, finish BWD profile ...
rank:34,optimizer_step time: 4.782080173492432
rank:34, finish optimizer.step profile ...
rank:34, Before memory release - Allocated: 2993059840, Reserved: 3076521984
rank:34, trace log has been written to txt...
rank:34, finish release GPU memory ...
rank:34, After memory release - Allocated: 910042112, Reserved: 1428160512
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 35, finish warm up ...
rank_id = 35, input_tensor_shapes: [(512, 1, 4096)]
rank:35,cuda fwd time: 6.384640216827393
rank:35, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425179878.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179878.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179879.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179880.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179881.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179881.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425179882.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425179883.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 35, finish FWD profile ...
rank:35, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425179885.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179888.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179891.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179893.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179896.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179898.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425179900.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425179903.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:35, finish BWD profile ...
rank:35,optimizer_step time: 4.838399887084961
rank:35, finish optimizer.step profile ...
rank:35, Before memory release - Allocated: 2993059840, Reserved: 3076521984
rank:35, trace log has been written to txt...
rank:35, finish release GPU memory ...
rank:35, After memory release - Allocated: 911022080, Reserved: 1386217472
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 36, finish warm up ...
rank_id = 36, input_tensor_shapes: [(512, 1, 4096)]
rank:36,cuda fwd time: 6.345727920532227
rank:36, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425180100.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180101.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180102.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180102.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180103.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180104.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180105.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180105.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 36, finish FWD profile ...
rank:36, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425180108.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180111.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180113.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180116.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180118.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180121.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180123.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180126.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:36, finish BWD profile ...
rank:36,optimizer_step time: 4.878335952758789
rank:36, finish optimizer.step profile ...
rank:36, Before memory release - Allocated: 2993059840, Reserved: 3076521984
rank:36, trace log has been written to txt...
rank:36, finish release GPU memory ...
rank:36, After memory release - Allocated: 910042112, Reserved: 1407188992
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 37, finish warm up ...
rank_id = 37, input_tensor_shapes: [(512, 1, 4096)]
rank:37,cuda fwd time: 6.99289608001709
rank:37, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425180327.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425180327.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425180329.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425180329.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425180330.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425180331.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425180332.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180332.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 37, finish FWD profile ...
rank:37, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425180335.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180338.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180340.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180343.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425180345.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180348.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425180350.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180353.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:37, finish BWD profile ...
rank:37,optimizer_step time: 4.86195182800293
rank:37, finish optimizer.step profile ...
rank:37, Before memory release - Allocated: 2992438784, Reserved: 3097493504
rank:37, trace log has been written to txt...
rank:37, finish release GPU memory ...
rank:37, After memory release - Allocated: 910401024, Reserved: 1428160512
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 38, finish warm up ...
rank_id = 38, input_tensor_shapes: [(512, 1, 4096)]
rank:38,cuda fwd time: 6.345727920532227
rank:38, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425180545.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180546.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180547.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180547.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180548.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180549.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180550.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180550.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 38, finish FWD profile ...
rank:38, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425180553.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180555.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180558.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180561.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180563.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180565.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180568.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180570.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:38, finish BWD profile ...
rank:38,optimizer_step time: 4.869120121002197
rank:38, finish optimizer.step profile ...
rank:38, Before memory release - Allocated: 2993059840, Reserved: 3076521984
rank:38, trace log has been written to txt...
rank:38, finish release GPU memory ...
rank:38, After memory release - Allocated: 910663168, Reserved: 1423966208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 1): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 39, finish warm up ...
rank_id = 39, input_tensor_shapes: [(512, 1, 4096)]
rank:39,cuda fwd time: 6.268928050994873
rank:39, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425180785.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180786.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180787.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180787.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180788.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180789.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180790.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425180790.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 39, finish FWD profile ...
rank:39, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425180793.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180796.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180798.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180801.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425180803.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180806.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425180808.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425180811.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:39, finish BWD profile ...
rank:39,optimizer_step time: 4.897791862487793
rank:39, finish optimizer.step profile ...
rank:39, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:39, trace log has been written to txt...
rank:39, finish release GPU memory ...
rank:39, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 40, finish warm up ...
rank_id = 40, input_tensor_shapes: [(512, 1, 4096)]
rank:40,cuda fwd time: 7.627776145935059
rank:40, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425181048.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181049.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181050.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181051.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181052.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181053.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181054.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181054.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 40, finish FWD profile ...
rank:40, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425181057.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181060.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181062.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181065.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181067.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181070.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425181072.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181075.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:40, finish BWD profile ...
rank:40,optimizer_step time: 4.889599800109863
rank:40, finish optimizer.step profile ...
rank:40, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:40, trace log has been written to txt...
rank:40, finish release GPU memory ...
rank:40, After memory release - Allocated: 910663168, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 41, finish warm up ...
rank_id = 41, input_tensor_shapes: [(512, 1, 4096)]
rank:41,cuda fwd time: 6.93555212020874
rank:41, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425181275.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181275.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181276.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181277.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181278.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181279.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181280.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181280.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 41, finish FWD profile ...
rank:41, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425181283.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181286.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181288.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181291.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181293.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181296.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181298.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181301.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:41, finish BWD profile ...
rank:41,optimizer_step time: 4.888576030731201
rank:41, finish optimizer.step profile ...
rank:41, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:41, trace log has been written to txt...
rank:41, finish release GPU memory ...
rank:41, After memory release - Allocated: 910401024, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 42, finish warm up ...
rank_id = 42, input_tensor_shapes: [(512, 1, 4096)]
rank:42,cuda fwd time: 6.792191982269287
rank:42, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425181496.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181496.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181497.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181498.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181499.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181499.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181500.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425181501.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 42, finish FWD profile ...
rank:42, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425181503.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181506.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181509.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181512.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181514.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181517.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181519.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181521.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:42, finish BWD profile ...
rank:42,optimizer_step time: 4.931583881378174
rank:42, finish optimizer.step profile ...
rank:42, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:42, trace log has been written to txt...
rank:42, finish release GPU memory ...
rank:42, After memory release - Allocated: 910663168, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 43, finish warm up ...
rank_id = 43, input_tensor_shapes: [(512, 1, 4096)]
rank:43,cuda fwd time: 7.748608112335205
rank:43, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425181748.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181748.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181750.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181750.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181752.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181752.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181753.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425181754.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 43, finish FWD profile ...
rank:43, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425181756.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181759.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181762.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181764.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181767.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181769.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425181772.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425181774.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:43, finish BWD profile ...
rank:43,optimizer_step time: 4.874239921569824
rank:43, finish optimizer.step profile ...
rank:43, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:43, trace log has been written to txt...
rank:43, finish release GPU memory ...
rank:43, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 44, finish warm up ...
rank_id = 44, input_tensor_shapes: [(512, 1, 4096)]
rank:44,cuda fwd time: 8.671232223510742
rank:44, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425182095.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182096.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182097.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182098.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182099.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182100.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182101.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182102.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 44, finish FWD profile ...
rank:44, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425182104.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182107.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182110.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182112.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182115.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182117.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425182120.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182122.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:44, finish BWD profile ...
rank:44,optimizer_step time: 5.002240180969238
rank:44, finish optimizer.step profile ...
rank:44, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:44, trace log has been written to txt...
rank:44, finish release GPU memory ...
rank:44, After memory release - Allocated: 910663168, Reserved: 1476395008
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 45, finish warm up ...
rank_id = 45, input_tensor_shapes: [(512, 1, 4096)]
rank:45,cuda fwd time: 6.311935901641846
rank:45, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425182316.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182317.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182318.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182318.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182319.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182320.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182321.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182321.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 45, finish FWD profile ...
rank:45, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425182324.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182327.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182329.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182332.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182334.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182337.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425182339.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182342.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:45, finish BWD profile ...
rank:45,optimizer_step time: 4.868095874786377
rank:45, finish optimizer.step profile ...
rank:45, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:45, trace log has been written to txt...
rank:45, finish release GPU memory ...
rank:45, After memory release - Allocated: 910401024, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 46, finish warm up ...
rank_id = 46, input_tensor_shapes: [(512, 1, 4096)]
rank:46,cuda fwd time: 7.158783912658691
rank:46, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425182553.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182554.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182555.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182556.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182557.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182557.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182558.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182559.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 46, finish FWD profile ...
rank:46, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425182562.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182564.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182567.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182570.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182572.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182575.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182577.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182580.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:46, finish BWD profile ...
rank:46,optimizer_step time: 4.869120121002197
rank:46, finish optimizer.step profile ...
rank:46, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:46, trace log has been written to txt...
rank:46, finish release GPU memory ...
rank:46, After memory release - Allocated: 910663168, Reserved: 1432354816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 47, finish warm up ...
rank_id = 47, input_tensor_shapes: [(512, 1, 4096)]
rank:47,cuda fwd time: 6.860799789428711
rank:47, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425182780.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182780.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182781.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182782.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182783.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182784.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182785.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182785.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 47, finish FWD profile ...
rank:47, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425182788.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182791.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182793.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425182796.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425182798.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182801.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425182803.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425182805.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:47, finish BWD profile ...
rank:47,optimizer_step time: 4.890624046325684
rank:47, finish optimizer.step profile ...
rank:47, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:47, trace log has been written to txt...
rank:47, finish release GPU memory ...
rank:47, After memory release - Allocated: 910401024, Reserved: 1426063360
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 48, finish warm up ...
rank_id = 48, input_tensor_shapes: [(512, 1, 4096)]
rank:48,cuda fwd time: 6.94271993637085
rank:48, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425182992.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182993.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182994.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182995.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182996.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425182996.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182997.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425182998.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 48, finish FWD profile ...
rank:48, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425183000.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183003.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183006.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183008.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183011.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183013.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425183016.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183018.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:48, finish BWD profile ...
rank:48,optimizer_step time: 4.884479999542236
rank:48, finish optimizer.step profile ...
rank:48, Before memory release - Allocated: 2993059840, Reserved: 3095396352
rank:48, trace log has been written to txt...
rank:48, finish release GPU memory ...
rank:48, After memory release - Allocated: 910663168, Reserved: 1405091840
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 49, finish warm up ...
rank_id = 49, input_tensor_shapes: [(512, 1, 4096)]
rank:49,cuda fwd time: 6.6017279624938965
rank:49, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425183205.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183206.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183207.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183207.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183208.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183209.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183210.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183210.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 49, finish FWD profile ...
rank:49, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425183213.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183216.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183218.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183221.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425183223.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183226.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425183228.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183231.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:49, finish BWD profile ...
rank:49,optimizer_step time: 4.996096134185791
rank:49, finish optimizer.step profile ...
rank:49, Before memory release - Allocated: 2993059840, Reserved: 3095396352
rank:49, trace log has been written to txt...
rank:49, finish release GPU memory ...
rank:49, After memory release - Allocated: 910401024, Reserved: 1405091840
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 50, finish warm up ...
rank_id = 50, input_tensor_shapes: [(512, 1, 4096)]
rank:50,cuda fwd time: 6.609920024871826
rank:50, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425183421.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183422.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183423.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183423.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183424.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183425.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183426.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183427.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 50, finish FWD profile ...
rank:50, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425183429.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183432.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183434.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183437.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183439.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183442.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425183444.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183447.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:50, finish BWD profile ...
rank:50,optimizer_step time: 4.8660478591918945
rank:50, finish optimizer.step profile ...
rank:50, Before memory release - Allocated: 2993059840, Reserved: 3093299200
rank:50, trace log has been written to txt...
rank:50, finish release GPU memory ...
rank:50, After memory release - Allocated: 910663168, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 51, finish warm up ...
rank_id = 51, input_tensor_shapes: [(512, 1, 4096)]
rank:51,cuda fwd time: 6.618112087249756
rank:51, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425183638.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183639.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183640.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183640.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183641.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183642.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183643.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425183644.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 51, finish FWD profile ...
rank:51, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425183646.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183649.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183652.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183654.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183657.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183659.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183662.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183664.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:51, finish BWD profile ...
rank:51,optimizer_step time: 4.840447902679443
rank:51, finish optimizer.step profile ...
rank:51, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:51, trace log has been written to txt...
rank:51, finish release GPU memory ...
rank:51, After memory release - Allocated: 910401024, Reserved: 1432354816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 52, finish warm up ...
rank_id = 52, input_tensor_shapes: [(512, 1, 4096)]
rank:52,cuda fwd time: 7.4352641105651855
rank:52, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425183885.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183886.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183887.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183887.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183888.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183889.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183890.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425183891.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 52, finish FWD profile ...
rank:52, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425183893.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183896.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183899.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183901.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183904.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183906.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183909.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425183911.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:52, finish BWD profile ...
rank:52,optimizer_step time: 4.948991775512695
rank:52, finish optimizer.step profile ...
rank:52, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:52, trace log has been written to txt...
rank:52, finish release GPU memory ...
rank:52, After memory release - Allocated: 910663168, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 53, finish warm up ...
rank_id = 53, input_tensor_shapes: [(512, 1, 4096)]
rank:53,cuda fwd time: 6.351871967315674
rank:53, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425184102.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184103.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184104.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184104.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184105.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184106.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184107.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184107.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 53, finish FWD profile ...
rank:53, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425184110.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184113.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184115.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184118.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184120.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425184123.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425184125.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184128.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:53, finish BWD profile ...
rank:53,optimizer_step time: 4.894720077514648
rank:53, finish optimizer.step profile ...
rank:53, Before memory release - Allocated: 2993780736, Reserved: 3072327680
rank:53, trace log has been written to txt...
rank:53, finish release GPU memory ...
rank:53, After memory release - Allocated: 910401024, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 54, finish warm up ...
rank_id = 54, input_tensor_shapes: [(512, 1, 4096)]
rank:54,cuda fwd time: 6.38156795501709
rank:54, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425184319.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184319.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184320.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184321.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184322.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184322.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184323.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184324.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 54, finish FWD profile ...
rank:54, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425184326.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184329.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184331.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184334.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184336.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425184339.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184341.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184344.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:54, finish BWD profile ...
rank:54,optimizer_step time: 4.8660478591918945
rank:54, finish optimizer.step profile ...
rank:54, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:54, trace log has been written to txt...
rank:54, finish release GPU memory ...
rank:54, After memory release - Allocated: 910663168, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 55, finish warm up ...
rank_id = 55, input_tensor_shapes: [(512, 1, 4096)]
rank:55,cuda fwd time: 6.773759841918945
rank:55, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425184532.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184532.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184533.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184534.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184535.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184536.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184537.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184537.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 55, finish FWD profile ...
rank:55, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425184540.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184543.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184545.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184548.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184551.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184553.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184556.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184558.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:55, finish BWD profile ...
rank:55,optimizer_step time: 4.905983924865723
rank:55, finish optimizer.step profile ...
rank:55, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:55, trace log has been written to txt...
rank:55, finish release GPU memory ...
rank:55, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 56, finish warm up ...
rank_id = 56, input_tensor_shapes: [(512, 1, 4096)]
rank:56,cuda fwd time: 7.185408115386963
rank:56, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425184760.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184760.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184761.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184762.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184763.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184764.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184765.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425184765.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 56, finish FWD profile ...
rank:56, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425184768.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184771.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184773.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184776.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425184778.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184781.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425184783.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184786.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:56, finish BWD profile ...
rank:56,optimizer_step time: 4.8957438468933105
rank:56, finish optimizer.step profile ...
rank:56, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:56, trace log has been written to txt...
rank:56, finish release GPU memory ...
rank:56, After memory release - Allocated: 910663168, Reserved: 1488977920
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 57, finish warm up ...
rank_id = 57, input_tensor_shapes: [(512, 1, 4096)]
rank:57,cuda fwd time: 6.3406081199646
rank:57, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425184971.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184972.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184973.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184973.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184974.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184975.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184976.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425184976.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 57, finish FWD profile ...
rank:57, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425184979.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184982.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184984.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184987.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184989.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184992.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425184994.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425184997.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:57, finish BWD profile ...
rank:57,optimizer_step time: 4.86297607421875
rank:57, finish optimizer.step profile ...
rank:57, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:57, trace log has been written to txt...
rank:57, finish release GPU memory ...
rank:57, After memory release - Allocated: 910401024, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 58, finish warm up ...
rank_id = 58, input_tensor_shapes: [(512, 1, 4096)]
rank:58,cuda fwd time: 6.717440128326416
rank:58, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425185190.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185191.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185192.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185192.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185193.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185194.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185195.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185196.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 58, finish FWD profile ...
rank:58, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425185198.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185201.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185203.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185206.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425185208.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185211.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425185213.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185216.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:58, finish BWD profile ...
rank:58,optimizer_step time: 4.90393590927124
rank:58, finish optimizer.step profile ...
rank:58, Before memory release - Allocated: 2993059840, Reserved: 3095396352
rank:58, trace log has been written to txt...
rank:58, finish release GPU memory ...
rank:58, After memory release - Allocated: 910663168, Reserved: 1426063360
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 59, finish warm up ...
rank_id = 59, input_tensor_shapes: [(512, 1, 4096)]
rank:59,cuda fwd time: 6.307839870452881
rank:59, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425185433.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185434.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185434.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185435.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185436.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185437.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185437.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185438.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 59, finish FWD profile ...
rank:59, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425185441.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185443.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185446.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185449.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425185451.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185454.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185456.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185459.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:59, finish BWD profile ...
rank:59,optimizer_step time: 4.863999843597412
rank:59, finish optimizer.step profile ...
rank:59, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:59, trace log has been written to txt...
rank:59, finish release GPU memory ...
rank:59, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 60, finish warm up ...
rank_id = 60, input_tensor_shapes: [(512, 1, 4096)]
rank:60,cuda fwd time: 7.19155216217041
rank:60, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425185670.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185671.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185672.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185673.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185674.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425185675.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185676.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185676.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 60, finish FWD profile ...
rank:60, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425185679.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185682.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185684.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185687.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185689.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185692.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185694.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185697.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:60, finish BWD profile ...
rank:60,optimizer_step time: 4.893695831298828
rank:60, finish optimizer.step profile ...
rank:60, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:60, trace log has been written to txt...
rank:60, finish release GPU memory ...
rank:60, After memory release - Allocated: 910663168, Reserved: 1488977920
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 61, finish warm up ...
rank_id = 61, input_tensor_shapes: [(512, 1, 4096)]
rank:61,cuda fwd time: 7.111680030822754
rank:61, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425185897.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185898.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185899.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185900.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185901.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185901.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185902.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425185903.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 61, finish FWD profile ...
rank:61, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425185905.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185908.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185911.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185913.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185916.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185918.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185921.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425185923.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:61, finish BWD profile ...
rank:61,optimizer_step time: 4.881408214569092
rank:61, finish optimizer.step profile ...
rank:61, Before memory release - Allocated: 2992535552, Reserved: 3120562176
rank:61, trace log has been written to txt...
rank:61, finish release GPU memory ...
rank:61, After memory release - Allocated: 910401024, Reserved: 1491075072
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 62, finish warm up ...
rank_id = 62, input_tensor_shapes: [(512, 1, 4096)]
rank:62,cuda fwd time: 6.3006720542907715
rank:62, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425186108.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186109.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186110.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186111.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186111.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186112.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186113.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186113.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 62, finish FWD profile ...
rank:62, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425186116.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186119.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186121.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186124.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186126.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186129.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425186131.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186134.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:62, finish BWD profile ...
rank:62,optimizer_step time: 4.846591949462891
rank:62, finish optimizer.step profile ...
rank:62, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:62, trace log has been written to txt...
rank:62, finish release GPU memory ...
rank:62, After memory release - Allocated: 910663168, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 63, finish warm up ...
rank_id = 63, input_tensor_shapes: [(512, 1, 4096)]
rank:63,cuda fwd time: 6.782976150512695
rank:63, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425186328.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186329.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186330.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186330.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186331.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186332.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186333.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186333.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 63, finish FWD profile ...
rank:63, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425186336.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186339.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186341.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186344.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186346.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186349.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425186351.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186354.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:63, finish BWD profile ...
rank:63,optimizer_step time: 4.846591949462891
rank:63, finish optimizer.step profile ...
rank:63, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:63, trace log has been written to txt...
rank:63, finish release GPU memory ...
rank:63, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 64, finish warm up ...
rank_id = 64, input_tensor_shapes: [(512, 1, 4096)]
rank:64,cuda fwd time: 7.517183780670166
rank:64, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425186582.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186583.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186584.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186585.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186586.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186587.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186588.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186589.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 64, finish FWD profile ...
rank:64, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425186591.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186594.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186596.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186599.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186601.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186604.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425186606.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186609.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:64, finish BWD profile ...
rank:64,optimizer_step time: 4.891647815704346
rank:64, finish optimizer.step profile ...
rank:64, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:64, trace log has been written to txt...
rank:64, finish release GPU memory ...
rank:64, After memory release - Allocated: 910663168, Reserved: 1497366528
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 65, finish warm up ...
rank_id = 65, input_tensor_shapes: [(512, 1, 4096)]
rank:65,cuda fwd time: 7.311359882354736
rank:65, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425186840.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186841.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186842.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186843.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186844.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425186844.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186845.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425186846.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 65, finish FWD profile ...
rank:65, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425186849.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186852.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186854.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186857.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186859.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186862.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186864.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425186867.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:65, finish BWD profile ...
rank:65,optimizer_step time: 4.876287937164307
rank:65, finish optimizer.step profile ...
rank:65, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:65, trace log has been written to txt...
rank:65, finish release GPU memory ...
rank:65, After memory release - Allocated: 910401024, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 66, finish warm up ...
rank_id = 66, input_tensor_shapes: [(512, 1, 4096)]
rank:66,cuda fwd time: 7.24889612197876
rank:66, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425187080.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187081.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187082.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187083.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187084.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187084.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187085.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187086.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 66, finish FWD profile ...
rank:66, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425187088.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187091.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187094.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187096.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187099.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187101.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425187104.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187106.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:66, finish BWD profile ...
rank:66,optimizer_step time: 4.889599800109863
rank:66, finish optimizer.step profile ...
rank:66, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:66, trace log has been written to txt...
rank:66, finish release GPU memory ...
rank:66, After memory release - Allocated: 910663168, Reserved: 1453326336
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 67, finish warm up ...
rank_id = 67, input_tensor_shapes: [(512, 1, 4096)]
rank:67,cuda fwd time: 6.351871967315674
rank:67, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425187330.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187330.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187331.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187332.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187333.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187333.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187334.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187335.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 67, finish FWD profile ...
rank:67, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425187337.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187340.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187342.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187345.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187347.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187350.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187353.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187355.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:67, finish BWD profile ...
rank:67,optimizer_step time: 4.881408214569092
rank:67, finish optimizer.step profile ...
rank:67, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:67, trace log has been written to txt...
rank:67, finish release GPU memory ...
rank:67, After memory release - Allocated: 910401024, Reserved: 1453326336
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 68, finish warm up ...
rank_id = 68, input_tensor_shapes: [(512, 1, 4096)]
rank:68,cuda fwd time: 6.627327919006348
rank:68, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425187541.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187542.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425187543.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187543.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187544.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187545.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187546.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187546.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 68, finish FWD profile ...
rank:68, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425187549.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187552.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187554.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187557.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187559.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187562.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425187564.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187567.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:68, finish BWD profile ...
rank:68,optimizer_step time: 4.835328102111816
rank:68, finish optimizer.step profile ...
rank:68, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:68, trace log has been written to txt...
rank:68, finish release GPU memory ...
rank:68, After memory release - Allocated: 910663168, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 69, finish warm up ...
rank_id = 69, input_tensor_shapes: [(512, 1, 4096)]
rank:69,cuda fwd time: 6.564864158630371
rank:69, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425187792.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187793.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187794.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187794.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187795.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187796.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187797.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425187797.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 69, finish FWD profile ...
rank:69, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425187800.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187803.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187805.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425187808.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425187810.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187813.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187815.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425187818.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:69, finish BWD profile ...
rank:69,optimizer_step time: 4.929535865783691
rank:69, finish optimizer.step profile ...
rank:69, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:69, trace log has been written to txt...
rank:69, finish release GPU memory ...
rank:69, After memory release - Allocated: 910401024, Reserved: 1426063360
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 70, finish warm up ...
rank_id = 70, input_tensor_shapes: [(512, 1, 4096)]
rank:70,cuda fwd time: 7.3758721351623535
rank:70, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425188024.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188025.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188026.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188026.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188027.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188028.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188029.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188030.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 70, finish FWD profile ...
rank:70, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425188032.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425188035.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188038.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188040.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188043.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188045.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425188047.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188050.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:70, finish BWD profile ...
rank:70,optimizer_step time: 4.856832027435303
rank:70, finish optimizer.step profile ...
rank:70, Before memory release - Allocated: 2993059840, Reserved: 3095396352
rank:70, trace log has been written to txt...
rank:70, finish release GPU memory ...
rank:70, After memory release - Allocated: 910663168, Reserved: 1426063360
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 2): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 71, finish warm up ...
rank_id = 71, input_tensor_shapes: [(512, 1, 4096)]
rank:71,cuda fwd time: 6.292479991912842
rank:71, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425188243.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188244.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188244.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188245.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188246.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188247.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188247.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188248.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 71, finish FWD profile ...
rank:71, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425188250.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188253.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188256.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188258.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425188261.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188263.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188266.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188268.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:71, finish BWD profile ...
rank:71,optimizer_step time: 4.848639965057373
rank:71, finish optimizer.step profile ...
rank:71, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:71, trace log has been written to txt...
rank:71, finish release GPU memory ...
rank:71, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 72, finish warm up ...
rank_id = 72, input_tensor_shapes: [(512, 1, 4096)]
rank:72,cuda fwd time: 6.988800048828125
rank:72, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425188454.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188455.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188456.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188456.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188457.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188458.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188459.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188460.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 72, finish FWD profile ...
rank:72, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425188462.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188465.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188467.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188470.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425188472.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425188475.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188477.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188480.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:72, finish BWD profile ...
rank:72,optimizer_step time: 4.9049601554870605
rank:72, finish optimizer.step profile ...
rank:72, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:72, trace log has been written to txt...
rank:72, finish release GPU memory ...
rank:72, After memory release - Allocated: 910663168, Reserved: 1488977920
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 73, finish warm up ...
rank_id = 73, input_tensor_shapes: [(512, 1, 4096)]
rank:73,cuda fwd time: 6.450175762176514
rank:73, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425188672.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188673.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188674.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188674.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188675.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188676.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188677.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188677.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 73, finish FWD profile ...
rank:73, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425188680.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188683.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188685.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188688.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188690.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188693.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425188695.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188698.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:73, finish BWD profile ...
rank:73,optimizer_step time: 4.831232070922852
rank:73, finish optimizer.step profile ...
rank:73, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:73, trace log has been written to txt...
rank:73, finish release GPU memory ...
rank:73, After memory release - Allocated: 910401024, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 74, finish warm up ...
rank_id = 74, input_tensor_shapes: [(512, 1, 4096)]
rank:74,cuda fwd time: 6.634496212005615
rank:74, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425188886.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188886.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188887.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188888.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188889.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188890.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425188891.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425188891.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 74, finish FWD profile ...
rank:74, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425188894.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188896.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425188899.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188902.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188904.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188907.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425188909.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425188912.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:74, finish BWD profile ...
rank:74,optimizer_step time: 4.856832027435303
rank:74, finish optimizer.step profile ...
rank:74, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:74, trace log has been written to txt...
rank:74, finish release GPU memory ...
rank:74, After memory release - Allocated: 910663168, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 75, finish warm up ...
rank_id = 75, input_tensor_shapes: [(512, 1, 4096)]
rank:75,cuda fwd time: 6.346752166748047
rank:75, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425189143.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425189144.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189145.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425189146.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425189146.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425189147.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425189148.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425189149.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 75, finish FWD profile ...
rank:75, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425189151.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189154.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189156.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189159.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189161.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189164.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425189166.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189169.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:75, finish BWD profile ...
rank:75,optimizer_step time: 4.9100799560546875
rank:75, finish optimizer.step profile ...
rank:75, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:75, trace log has been written to txt...
rank:75, finish release GPU memory ...
rank:75, After memory release - Allocated: 910401024, Reserved: 1488977920
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 76, finish warm up ...
rank_id = 76, input_tensor_shapes: [(512, 1, 4096)]
rank:76,cuda fwd time: 7.385087966918945
rank:76, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425189379.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189379.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189380.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189381.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189382.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189383.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189384.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189385.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 76, finish FWD profile ...
rank:76, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425189387.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189390.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189392.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189395.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425189397.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189400.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425189402.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189405.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:76, finish BWD profile ...
rank:76,optimizer_step time: 4.91315221786499
rank:76, finish optimizer.step profile ...
rank:76, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:76, trace log has been written to txt...
rank:76, finish release GPU memory ...
rank:76, After memory release - Allocated: 910663168, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 77, finish warm up ...
rank_id = 77, input_tensor_shapes: [(512, 1, 4096)]
rank:77,cuda fwd time: 8.09779167175293
rank:77, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425189761.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189762.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189764.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189764.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189765.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189766.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189767.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189768.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 77, finish FWD profile ...
rank:77, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425189771.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189773.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189776.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189779.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189781.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189783.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189786.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425189789.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:77, finish BWD profile ...
rank:77,optimizer_step time: 4.915200233459473
rank:77, finish optimizer.step profile ...
rank:77, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:77, trace log has been written to txt...
rank:77, finish release GPU memory ...
rank:77, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 78, finish warm up ...
rank_id = 78, input_tensor_shapes: [(512, 1, 4096)]
rank:78,cuda fwd time: 7.343103885650635
rank:78, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425189999.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425189999.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190000.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190001.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190002.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190003.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190004.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190005.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 78, finish FWD profile ...
rank:78, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425190007.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190010.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190012.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190015.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190017.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190020.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425190022.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190025.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:78, finish BWD profile ...
rank:78,optimizer_step time: 4.8803839683532715
rank:78, finish optimizer.step profile ...
rank:78, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:78, trace log has been written to txt...
rank:78, finish release GPU memory ...
rank:78, After memory release - Allocated: 910663168, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 79, finish warm up ...
rank_id = 79, input_tensor_shapes: [(512, 1, 4096)]
rank:79,cuda fwd time: 6.228991985321045
rank:79, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425190223.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190223.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190224.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190225.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190226.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190226.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190227.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425190228.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 79, finish FWD profile ...
rank:79, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425190230.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190233.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425190235.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190238.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190240.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190243.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425190245.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190248.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:79, finish BWD profile ...
rank:79,optimizer_step time: 4.870143890380859
rank:79, finish optimizer.step profile ...
rank:79, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:79, trace log has been written to txt...
rank:79, finish release GPU memory ...
rank:79, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 80, finish warm up ...
rank_id = 80, input_tensor_shapes: [(512, 1, 4096)]
rank:80,cuda fwd time: 7.349247932434082
rank:80, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425190463.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190463.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190464.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190465.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190466.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190467.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190468.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190469.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 80, finish FWD profile ...
rank:80, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425190471.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190474.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190476.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190479.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425190481.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190484.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425190486.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190489.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:80, finish BWD profile ...
rank:80,optimizer_step time: 4.863999843597412
rank:80, finish optimizer.step profile ...
rank:80, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:80, trace log has been written to txt...
rank:80, finish release GPU memory ...
rank:80, After memory release - Allocated: 910663168, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 81, finish warm up ...
rank_id = 81, input_tensor_shapes: [(512, 1, 4096)]
rank:81,cuda fwd time: 7.120895862579346
rank:81, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425190696.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190697.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190698.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190699.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190700.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190700.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190701.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190702.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 81, finish FWD profile ...
rank:81, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425190705.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190707.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190710.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190713.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190715.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190717.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190720.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190723.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:81, finish BWD profile ...
rank:81,optimizer_step time: 4.879360198974609
rank:81, finish optimizer.step profile ...
rank:81, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:81, trace log has been written to txt...
rank:81, finish release GPU memory ...
rank:81, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 82, finish warm up ...
rank_id = 82, input_tensor_shapes: [(512, 1, 4096)]
rank:82,cuda fwd time: 7.293951988220215
rank:82, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425190937.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190938.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190939.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190940.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190941.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190942.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190943.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425190943.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 82, finish FWD profile ...
rank:82, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425190946.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190949.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190951.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190954.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425190956.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190959.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190961.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425190964.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:82, finish BWD profile ...
rank:82,optimizer_step time: 4.86195182800293
rank:82, finish optimizer.step profile ...
rank:82, Before memory release - Allocated: 2993059840, Reserved: 3072327680
rank:82, trace log has been written to txt...
rank:82, finish release GPU memory ...
rank:82, After memory release - Allocated: 910663168, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 83, finish warm up ...
rank_id = 83, input_tensor_shapes: [(512, 1, 4096)]
rank:83,cuda fwd time: 6.383615970611572
rank:83, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425191152.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191153.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191154.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191155.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191155.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191156.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191157.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425191158.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 83, finish FWD profile ...
rank:83, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425191161.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191164.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191166.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191169.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191172.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191174.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191177.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191179.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:83, finish BWD profile ...
rank:83,optimizer_step time: 4.870143890380859
rank:83, finish optimizer.step profile ...
rank:83, Before memory release - Allocated: 2992535552, Reserved: 3118465024
rank:83, trace log has been written to txt...
rank:83, finish release GPU memory ...
rank:83, After memory release - Allocated: 910401024, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 84, finish warm up ...
rank_id = 84, input_tensor_shapes: [(512, 1, 4096)]
rank:84,cuda fwd time: 7.311359882354736
rank:84, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425191384.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191385.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191386.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191387.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191388.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191388.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191389.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191390.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 84, finish FWD profile ...
rank:84, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425191393.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191395.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191398.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191401.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425191403.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191406.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425191408.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191411.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:84, finish BWD profile ...
rank:84,optimizer_step time: 4.885503768920898
rank:84, finish optimizer.step profile ...
rank:84, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:84, trace log has been written to txt...
rank:84, finish release GPU memory ...
rank:84, After memory release - Allocated: 910663168, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 85, finish warm up ...
rank_id = 85, input_tensor_shapes: [(512, 1, 4096)]
rank:85,cuda fwd time: 7.303167819976807
rank:85, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425191622.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191622.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191623.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191624.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191625.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191626.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191627.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191628.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 85, finish FWD profile ...
rank:85, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425191630.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191633.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191635.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191638.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191640.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191643.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425191645.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191648.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:85, finish BWD profile ...
rank:85,optimizer_step time: 4.887551784515381
rank:85, finish optimizer.step profile ...
rank:85, Before memory release - Allocated: 2993059840, Reserved: 3095396352
rank:85, trace log has been written to txt...
rank:85, finish release GPU memory ...
rank:85, After memory release - Allocated: 910401024, Reserved: 1426063360
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 86, finish warm up ...
rank_id = 86, input_tensor_shapes: [(512, 1, 4096)]
rank:86,cuda fwd time: 7.566336154937744
rank:86, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425191860.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191861.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191862.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191863.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191864.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191865.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191866.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425191866.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 86, finish FWD profile ...
rank:86, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425191869.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191872.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191874.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191877.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191879.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191882.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191884.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425191887.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:86, finish BWD profile ...
rank:86,optimizer_step time: 4.898816108703613
rank:86, finish optimizer.step profile ...
rank:86, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:86, trace log has been written to txt...
rank:86, finish release GPU memory ...
rank:86, After memory release - Allocated: 910663168, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 87, finish warm up ...
rank_id = 87, input_tensor_shapes: [(512, 1, 4096)]
rank:87,cuda fwd time: 7.297023773193359
rank:87, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425192094.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192095.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192096.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192097.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192098.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192099.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192100.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192100.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 87, finish FWD profile ...
rank:87, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425192103.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192106.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192108.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192111.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192113.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192116.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425192118.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192121.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:87, finish BWD profile ...
rank:87,optimizer_step time: 4.877312183380127
rank:87, finish optimizer.step profile ...
rank:87, Before memory release - Allocated: 2993059840, Reserved: 3074424832
rank:87, trace log has been written to txt...
rank:87, finish release GPU memory ...
rank:87, After memory release - Allocated: 910401024, Reserved: 1455423488
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 88, finish warm up ...
rank_id = 88, input_tensor_shapes: [(512, 1, 4096)]
rank:88,cuda fwd time: 7.35641622543335
rank:88, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425192319.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192320.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192321.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192322.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192323.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192323.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192324.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192325.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 88, finish FWD profile ...
rank:88, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425192328.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192330.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192333.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192336.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192338.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192341.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192343.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192346.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:88, finish BWD profile ...
rank:88,optimizer_step time: 7.342080116271973
rank:88, finish optimizer.step profile ...
rank:88, Before memory release - Allocated: 2993584128, Reserved: 3074424832
rank:88, trace log has been written to txt...
rank:88, finish release GPU memory ...
rank:88, After memory release - Allocated: 911187456, Reserved: 1455423488
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 89, finish warm up ...
rank_id = 89, input_tensor_shapes: [(512, 1, 4096)]
rank:89,cuda fwd time: 7.85203218460083
rank:89, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425192555.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192556.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192557.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192557.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192559.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192559.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192560.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192561.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 89, finish FWD profile ...
rank:89, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425192563.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192566.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192569.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192571.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192574.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192576.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425192579.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192581.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:89, finish BWD profile ...
rank:89,optimizer_step time: 4.919295787811279
rank:89, finish optimizer.step profile ...
rank:89, Before memory release - Allocated: 2993584128, Reserved: 3074424832
rank:89, trace log has been written to txt...
rank:89, finish release GPU memory ...
rank:89, After memory release - Allocated: 910401024, Reserved: 1447034880
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 90, finish warm up ...
rank_id = 90, input_tensor_shapes: [(512, 1, 4096)]
rank:90,cuda fwd time: 7.355391979217529
rank:90, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425192779.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192780.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192781.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192781.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192782.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192783.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192784.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425192785.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 90, finish FWD profile ...
rank:90, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.08,timestamp=2425192787.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192790.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192792.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425192795.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192797.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192800.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425192802.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425192805.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:90, finish BWD profile ...
rank:90,optimizer_step time: 4.91315221786499
rank:90, finish optimizer.step profile ...
rank:90, Before memory release - Allocated: 2993584128, Reserved: 3074424832
rank:90, trace log has been written to txt...
rank:90, finish release GPU memory ...
rank:90, After memory release - Allocated: 911187456, Reserved: 1488977920
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 91, finish warm up ...
rank_id = 91, input_tensor_shapes: [(512, 1, 4096)]
rank:91,cuda fwd time: 8.338432312011719
rank:91, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425193157.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193158.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193159.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193160.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193161.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193162.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193163.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193164.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 91, finish FWD profile ...
rank:91, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425193167.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193169.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193172.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193175.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193177.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425193179.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425193182.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193184.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:91, finish BWD profile ...
rank:91,optimizer_step time: 4.923391819000244
rank:91, finish optimizer.step profile ...
rank:91, Before memory release - Allocated: 2993584128, Reserved: 3074424832
rank:91, trace log has been written to txt...
rank:91, finish release GPU memory ...
rank:91, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 92, finish warm up ...
rank_id = 92, input_tensor_shapes: [(512, 1, 4096)]
rank:92,cuda fwd time: 7.742464065551758
rank:92, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425193395.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193396.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193397.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193397.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193398.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193399.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193400.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193401.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 92, finish FWD profile ...
rank:92, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425193404.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193406.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193409.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193412.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193414.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193417.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425193419.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193421.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:92, finish BWD profile ...
rank:92,optimizer_step time: 4.959231853485107
rank:92, finish optimizer.step profile ...
rank:92, Before memory release - Allocated: 2993059840, Reserved: 3118465024
rank:92, trace log has been written to txt...
rank:92, finish release GPU memory ...
rank:92, After memory release - Allocated: 911187456, Reserved: 1468006400
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 93, finish warm up ...
rank_id = 93, input_tensor_shapes: [(512, 1, 4096)]
rank:93,cuda fwd time: 7.522304058074951
rank:93, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425193640.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193641.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193642.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193643.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193644.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193644.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193645.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193646.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 93, finish FWD profile ...
rank:93, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425193649.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193651.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193654.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193657.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193659.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193662.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193664.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193667.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:93, finish BWD profile ...
rank:93,optimizer_step time: 4.885503768920898
rank:93, finish optimizer.step profile ...
rank:93, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:93, trace log has been written to txt...
rank:93, finish release GPU memory ...
rank:93, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 94, finish warm up ...
rank_id = 94, input_tensor_shapes: [(512, 1, 4096)]
rank:94,cuda fwd time: 7.540736198425293
rank:94, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425193884.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193885.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193886.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193887.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193888.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193889.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193890.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425193890.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 94, finish FWD profile ...
rank:94, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425193893.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193896.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193898.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193901.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425193903.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193906.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425193908.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425193911.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:94, finish BWD profile ...
rank:94,optimizer_step time: 4.882431983947754
rank:94, finish optimizer.step profile ...
rank:94, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:94, trace log has been written to txt...
rank:94, finish release GPU memory ...
rank:94, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 95, finish warm up ...
rank_id = 95, input_tensor_shapes: [(512, 1, 4096)]
rank:95,cuda fwd time: 7.379968166351318
rank:95, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425194130.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194131.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194132.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194133.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194134.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194134.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194135.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194136.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 95, finish FWD profile ...
rank:95, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425194139.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194141.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194144.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194147.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425194149.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194152.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194154.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194157.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:95, finish BWD profile ...
rank:95,optimizer_step time: 4.899839878082275
rank:95, finish optimizer.step profile ...
rank:95, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:95, trace log has been written to txt...
rank:95, finish release GPU memory ...
rank:95, After memory release - Allocated: 910401024, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 96, finish warm up ...
rank_id = 96, input_tensor_shapes: [(512, 1, 4096)]
rank:96,cuda fwd time: 7.320576190948486
rank:96, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425194383.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194384.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194385.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194386.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194387.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194387.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194388.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194389.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 96, finish FWD profile ...
rank:96, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425194392.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194394.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194397.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194400.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425194402.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194405.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194407.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194410.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:96, finish BWD profile ...
rank:96,optimizer_step time: 4.9694719314575195
rank:96, finish optimizer.step profile ...
rank:96, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:96, trace log has been written to txt...
rank:96, finish release GPU memory ...
rank:96, After memory release - Allocated: 911187456, Reserved: 1507852288
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 97, finish warm up ...
rank_id = 97, input_tensor_shapes: [(512, 1, 4096)]
rank:97,cuda fwd time: 7.690239906311035
rank:97, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425194647.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194648.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194649.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194650.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194651.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194652.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194653.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194653.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 97, finish FWD profile ...
rank:97, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425194656.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194659.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194661.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194664.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194666.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194669.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425194671.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194674.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:97, finish BWD profile ...
rank:97,optimizer_step time: 4.933631896972656
rank:97, finish optimizer.step profile ...
rank:97, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:97, trace log has been written to txt...
rank:97, finish release GPU memory ...
rank:97, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 98, finish warm up ...
rank_id = 98, input_tensor_shapes: [(512, 1, 4096)]
rank:98,cuda fwd time: 7.358463764190674
rank:98, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425194873.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194874.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194875.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194876.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194877.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194877.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194878.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425194879.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 98, finish FWD profile ...
rank:98, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425194881.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194884.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194887.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194889.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194892.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194895.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194897.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425194900.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:98, finish BWD profile ...
rank:98,optimizer_step time: 4.884479999542236
rank:98, finish optimizer.step profile ...
rank:98, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:98, trace log has been written to txt...
rank:98, finish release GPU memory ...
rank:98, After memory release - Allocated: 911187456, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 99, finish warm up ...
rank_id = 99, input_tensor_shapes: [(512, 1, 4096)]
rank:99,cuda fwd time: 7.3021440505981445
rank:99, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425195116.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195117.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195118.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195119.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195120.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195121.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195122.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195122.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 99, finish FWD profile ...
rank:99, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425195125.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195128.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195130.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195133.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195135.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195138.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195140.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195143.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:99, finish BWD profile ...
rank:99,optimizer_step time: 4.921343803405762
rank:99, finish optimizer.step profile ...
rank:99, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:99, trace log has been written to txt...
rank:99, finish release GPU memory ...
rank:99, After memory release - Allocated: 910401024, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 100, finish warm up ...
rank_id = 100, input_tensor_shapes: [(512, 1, 4096)]
rank:100,cuda fwd time: 8.758272171020508
rank:100, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425195349.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195350.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195351.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195352.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195353.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195354.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195355.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425195357.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 100, finish FWD profile ...
rank:100, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425195359.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195362.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195365.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195368.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425195370.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195373.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195375.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195378.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:100, finish BWD profile ...
rank:100,optimizer_step time: 4.973567962646484
rank:100, finish optimizer.step profile ...
rank:100, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:100, trace log has been written to txt...
rank:100, finish release GPU memory ...
rank:100, After memory release - Allocated: 911187456, Reserved: 1472200704
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 101, finish warm up ...
rank_id = 101, input_tensor_shapes: [(512, 1, 4096)]
rank:101,cuda fwd time: 7.328767776489258
rank:101, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425195577.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195578.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195579.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195579.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195580.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195581.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195582.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195583.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 101, finish FWD profile ...
rank:101, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425195585.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195588.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195590.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195593.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425195595.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195598.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195600.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195603.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:101, finish BWD profile ...
rank:101,optimizer_step time: 4.927487850189209
rank:101, finish optimizer.step profile ...
rank:101, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:101, trace log has been written to txt...
rank:101, finish release GPU memory ...
rank:101, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 102, finish warm up ...
rank_id = 102, input_tensor_shapes: [(512, 1, 4096)]
rank:102,cuda fwd time: 7.345151901245117
rank:102, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425195822.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195823.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195824.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195824.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195825.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195826.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195827.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425195828.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 102, finish FWD profile ...
rank:102, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425195830.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195833.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195835.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195838.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195841.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195843.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195846.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425195848.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:102, finish BWD profile ...
rank:102,optimizer_step time: 4.916224002838135
rank:102, finish optimizer.step profile ...
rank:102, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:102, trace log has been written to txt...
rank:102, finish release GPU memory ...
rank:102, After memory release - Allocated: 911187456, Reserved: 1423966208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 3): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 103, finish warm up ...
rank_id = 103, input_tensor_shapes: [(512, 1, 4096)]
rank:103,cuda fwd time: 7.3021440505981445
rank:103, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425196071.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196071.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196072.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196073.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196074.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196075.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196076.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196077.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 103, finish FWD profile ...
rank:103, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425196079.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196082.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196084.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196087.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196089.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196092.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196094.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196097.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:103, finish BWD profile ...
rank:103,optimizer_step time: 4.902912139892578
rank:103, finish optimizer.step profile ...
rank:103, Before memory release - Allocated: 2994272256, Reserved: 3091202048
rank:103, trace log has been written to txt...
rank:103, finish release GPU memory ...
rank:103, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 104, finish warm up ...
rank_id = 104, input_tensor_shapes: [(512, 1, 4096)]
rank:104,cuda fwd time: 7.3164801597595215
rank:104, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425196293.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196294.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196295.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196296.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196297.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196297.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196298.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196299.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 104, finish FWD profile ...
rank:104, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425196301.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196304.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196307.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196309.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196312.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196314.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196317.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196320.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:104, finish BWD profile ...
rank:104,optimizer_step time: 4.891647815704346
rank:104, finish optimizer.step profile ...
rank:104, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:104, trace log has been written to txt...
rank:104, finish release GPU memory ...
rank:104, After memory release - Allocated: 911187456, Reserved: 1484783616
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 105, finish warm up ...
rank_id = 105, input_tensor_shapes: [(512, 1, 4096)]
rank:105,cuda fwd time: 7.932928085327148
rank:105, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425196526.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196527.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196528.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196529.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196530.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196531.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196532.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196533.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 105, finish FWD profile ...
rank:105, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425196535.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196538.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196541.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196543.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196546.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196548.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196551.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196553.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:105, finish BWD profile ...
rank:105,optimizer_step time: 4.9244160652160645
rank:105, finish optimizer.step profile ...
rank:105, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:105, trace log has been written to txt...
rank:105, finish release GPU memory ...
rank:105, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 106, finish warm up ...
rank_id = 106, input_tensor_shapes: [(512, 1, 4096)]
rank:106,cuda fwd time: 7.552000045776367
rank:106, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425196748.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196748.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196749.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196750.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196751.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196752.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196753.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196754.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 106, finish FWD profile ...
rank:106, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.08,timestamp=2425196756.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196759.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196762.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425196764.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196767.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196769.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425196772.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425196774.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:106, finish BWD profile ...
rank:106,optimizer_step time: 4.911104202270508
rank:106, finish optimizer.step profile ...
rank:106, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:106, trace log has been written to txt...
rank:106, finish release GPU memory ...
rank:106, After memory release - Allocated: 911187456, Reserved: 1432354816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 107, finish warm up ...
rank_id = 107, input_tensor_shapes: [(512, 1, 4096)]
rank:107,cuda fwd time: 7.478271961212158
rank:107, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425196993.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196993.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196994.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196995.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196996.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196997.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196998.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425196999.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 107, finish FWD profile ...
rank:107, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425197001.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197004.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197006.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197009.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197011.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197014.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197016.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197019.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:107, finish BWD profile ...
rank:107,optimizer_step time: 4.905983924865723
rank:107, finish optimizer.step profile ...
rank:107, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:107, trace log has been written to txt...
rank:107, finish release GPU memory ...
rank:107, After memory release - Allocated: 910401024, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 108, finish warm up ...
rank_id = 108, input_tensor_shapes: [(512, 1, 4096)]
rank:108,cuda fwd time: 9.21190357208252
rank:108, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425197451.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197452.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197453.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197454.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197455.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197456.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197457.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197458.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 108, finish FWD profile ...
rank:108, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425197461.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197464.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197466.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197469.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425197471.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197474.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197476.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197479.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:108, finish BWD profile ...
rank:108,optimizer_step time: 4.989952087402344
rank:108, finish optimizer.step profile ...
rank:108, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:108, trace log has been written to txt...
rank:108, finish release GPU memory ...
rank:108, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 109, finish warm up ...
rank_id = 109, input_tensor_shapes: [(512, 1, 4096)]
rank:109,cuda fwd time: 8.18073558807373
rank:109, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425197684.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197685.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197686.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197687.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197688.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197689.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197690.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197691.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 109, finish FWD profile ...
rank:109, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425197693.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197696.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197698.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197701.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197703.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197706.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425197708.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197711.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:109, finish BWD profile ...
rank:109,optimizer_step time: 4.91212797164917
rank:109, finish optimizer.step profile ...
rank:109, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:109, trace log has been written to txt...
rank:109, finish release GPU memory ...
rank:109, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 110, finish warm up ...
rank_id = 110, input_tensor_shapes: [(512, 1, 4096)]
rank:110,cuda fwd time: 8.073216438293457
rank:110, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425197920.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197921.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197922.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197923.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197924.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197925.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197926.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425197927.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 110, finish FWD profile ...
rank:110, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425197929.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197932.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197934.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197937.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425197940.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197942.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425197944.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425197947.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:110, finish BWD profile ...
rank:110,optimizer_step time: 4.9049601554870605
rank:110, finish optimizer.step profile ...
rank:110, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:110, trace log has been written to txt...
rank:110, finish release GPU memory ...
rank:110, After memory release - Allocated: 911187456, Reserved: 1432354816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 111, finish warm up ...
rank_id = 111, input_tensor_shapes: [(512, 1, 4096)]
rank:111,cuda fwd time: 7.816192150115967
rank:111, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425198158.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198159.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198160.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198161.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198162.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198163.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198164.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198165.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 111, finish FWD profile ...
rank:111, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425198167.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198170.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198172.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198175.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425198177.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198180.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198182.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198185.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:111, finish BWD profile ...
rank:111,optimizer_step time: 4.914175987243652
rank:111, finish optimizer.step profile ...
rank:111, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:111, trace log has been written to txt...
rank:111, finish release GPU memory ...
rank:111, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 112, finish warm up ...
rank_id = 112, input_tensor_shapes: [(512, 1, 4096)]
rank:112,cuda fwd time: 7.368703842163086
rank:112, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425198384.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198385.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198386.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198387.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198388.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198389.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198390.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198390.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 112, finish FWD profile ...
rank:112, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425198393.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198396.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198398.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198401.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198403.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198406.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198408.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198411.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:112, finish BWD profile ...
rank:112,optimizer_step time: 4.876287937164307
rank:112, finish optimizer.step profile ...
rank:112, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:112, trace log has been written to txt...
rank:112, finish release GPU memory ...
rank:112, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 113, finish warm up ...
rank_id = 113, input_tensor_shapes: [(512, 1, 4096)]
rank:113,cuda fwd time: 7.33081579208374
rank:113, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425198608.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198609.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198610.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198611.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198612.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198612.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198613.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198614.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 113, finish FWD profile ...
rank:113, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425198617.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198619.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198622.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198624.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198627.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198629.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198632.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198634.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:113, finish BWD profile ...
rank:113,optimizer_step time: 4.878335952758789
rank:113, finish optimizer.step profile ...
rank:113, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:113, trace log has been written to txt...
rank:113, finish release GPU memory ...
rank:113, After memory release - Allocated: 910401024, Reserved: 1423966208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 114, finish warm up ...
rank_id = 114, input_tensor_shapes: [(512, 1, 4096)]
rank:114,cuda fwd time: 7.379968166351318
rank:114, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425198830.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198831.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198832.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198832.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198833.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198834.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198835.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425198836.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 114, finish FWD profile ...
rank:114, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425198838.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198841.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198843.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198846.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425198848.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198851.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198853.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425198856.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:114, finish BWD profile ...
rank:114,optimizer_step time: 4.905983924865723
rank:114, finish optimizer.step profile ...
rank:114, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:114, trace log has been written to txt...
rank:114, finish release GPU memory ...
rank:114, After memory release - Allocated: 911187456, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 115, finish warm up ...
rank_id = 115, input_tensor_shapes: [(512, 1, 4096)]
rank:115,cuda fwd time: 7.758848190307617
rank:115, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425199072.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199073.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199074.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199075.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199076.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199077.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199078.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199079.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 115, finish FWD profile ...
rank:115, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425199081.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199084.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199086.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199089.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199091.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199094.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199096.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199099.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:115, finish BWD profile ...
rank:115,optimizer_step time: 4.8803839683532715
rank:115, finish optimizer.step profile ...
rank:115, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:115, trace log has been written to txt...
rank:115, finish release GPU memory ...
rank:115, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 116, finish warm up ...
rank_id = 116, input_tensor_shapes: [(512, 1, 4096)]
rank:116,cuda fwd time: 8.252415657043457
rank:116, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425199314.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199315.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199316.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199317.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199318.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199319.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199320.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199321.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 116, finish FWD profile ...
rank:116, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425199324.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199326.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199329.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199332.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199334.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199337.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199339.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199342.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:116, finish BWD profile ...
rank:116,optimizer_step time: 4.951039791107178
rank:116, finish optimizer.step profile ...
rank:116, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:116, trace log has been written to txt...
rank:116, finish release GPU memory ...
rank:116, After memory release - Allocated: 911187456, Reserved: 1495269376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 117, finish warm up ...
rank_id = 117, input_tensor_shapes: [(512, 1, 4096)]
rank:117,cuda fwd time: 7.79366397857666
rank:117, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425199602.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199603.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199604.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199604.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199606.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199606.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199607.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199608.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 117, finish FWD profile ...
rank:117, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425199611.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199613.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199616.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199619.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199621.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199624.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199626.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199629.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:117, finish BWD profile ...
rank:117,optimizer_step time: 4.916224002838135
rank:117, finish optimizer.step profile ...
rank:117, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:117, trace log has been written to txt...
rank:117, finish release GPU memory ...
rank:117, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 118, finish warm up ...
rank_id = 118, input_tensor_shapes: [(512, 1, 4096)]
rank:118,cuda fwd time: 7.808000087738037
rank:118, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425199859.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199860.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199861.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199862.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199863.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199864.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199865.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425199866.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 118, finish FWD profile ...
rank:118, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425199868.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425199871.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199874.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199876.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199879.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199881.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425199884.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425199886.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:118, finish BWD profile ...
rank:118,optimizer_step time: 4.900864124298096
rank:118, finish optimizer.step profile ...
rank:118, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:118, trace log has been written to txt...
rank:118, finish release GPU memory ...
rank:118, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 119, finish warm up ...
rank_id = 119, input_tensor_shapes: [(512, 1, 4096)]
rank:119,cuda fwd time: 7.355391979217529
rank:119, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425200079.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200080.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200081.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200082.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200083.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200083.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200084.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200085.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 119, finish FWD profile ...
rank:119, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425200087.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200090.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200093.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200095.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200098.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200100.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200103.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200105.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:119, finish BWD profile ...
rank:119,optimizer_step time: 4.919295787811279
rank:119, finish optimizer.step profile ...
rank:119, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:119, trace log has been written to txt...
rank:119, finish release GPU memory ...
rank:119, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 120, finish warm up ...
rank_id = 120, input_tensor_shapes: [(512, 1, 4096)]
rank:120,cuda fwd time: 7.254015922546387
rank:120, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425200326.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200327.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200328.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200329.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200330.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200330.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200331.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200332.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 120, finish FWD profile ...
rank:120, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425200334.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200337.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200340.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200342.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425200345.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200347.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200350.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200352.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:120, finish BWD profile ...
rank:120,optimizer_step time: 4.914175987243652
rank:120, finish optimizer.step profile ...
rank:120, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:120, trace log has been written to txt...
rank:120, finish release GPU memory ...
rank:120, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 121, finish warm up ...
rank_id = 121, input_tensor_shapes: [(512, 1, 4096)]
rank:121,cuda fwd time: 8.79923152923584
rank:121, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425200732.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200732.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200734.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200735.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200736.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200737.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200738.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200739.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 121, finish FWD profile ...
rank:121, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425200741.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200744.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200746.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200749.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200752.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200754.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200756.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425200759.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:121, finish BWD profile ...
rank:121,optimizer_step time: 4.885503768920898
rank:121, finish optimizer.step profile ...
rank:121, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:121, trace log has been written to txt...
rank:121, finish release GPU memory ...
rank:121, After memory release - Allocated: 910401024, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 122, finish warm up ...
rank_id = 122, input_tensor_shapes: [(512, 1, 4096)]
rank:122,cuda fwd time: 7.386112213134766
rank:122, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425200979.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200979.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200980.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200981.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200982.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200983.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200984.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425200985.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 122, finish FWD profile ...
rank:122, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425200987.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200990.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200992.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200995.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425200997.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201000.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201002.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201005.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:122, finish BWD profile ...
rank:122,optimizer_step time: 4.911104202270508
rank:122, finish optimizer.step profile ...
rank:122, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:122, trace log has been written to txt...
rank:122, finish release GPU memory ...
rank:122, After memory release - Allocated: 911187456, Reserved: 1451229184
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 123, finish warm up ...
rank_id = 123, input_tensor_shapes: [(512, 1, 4096)]
rank:123,cuda fwd time: 7.273471832275391
rank:123, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425201203.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201204.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201205.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201206.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201207.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201207.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201208.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425201209.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 123, finish FWD profile ...
rank:123, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425201211.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201214.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201217.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201219.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201222.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201224.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201227.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201229.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:123, finish BWD profile ...
rank:123,optimizer_step time: 4.884479999542236
rank:123, finish optimizer.step profile ...
rank:123, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:123, trace log has been written to txt...
rank:123, finish release GPU memory ...
rank:123, After memory release - Allocated: 910401024, Reserved: 1451229184
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 124, finish warm up ...
rank_id = 124, input_tensor_shapes: [(512, 1, 4096)]
rank:124,cuda fwd time: 7.332863807678223
rank:124, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425201444.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201445.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201446.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201447.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201448.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201449.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201450.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201450.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 124, finish FWD profile ...
rank:124, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425201453.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201456.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201458.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425201461.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201463.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201466.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201468.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201471.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:124, finish BWD profile ...
rank:124,optimizer_step time: 4.889599800109863
rank:124, finish optimizer.step profile ...
rank:124, Before memory release - Allocated: 2993584128, Reserved: 3091202048
rank:124, trace log has been written to txt...
rank:124, finish release GPU memory ...
rank:124, After memory release - Allocated: 911187456, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 125, finish warm up ...
rank_id = 125, input_tensor_shapes: [(512, 1, 4096)]
rank:125,cuda fwd time: 7.450623989105225
rank:125, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425201666.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201667.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201668.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201669.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201670.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201671.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201672.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201672.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 125, finish FWD profile ...
rank:125, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425201675.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201678.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201680.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201683.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201685.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201688.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201690.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201693.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:125, finish BWD profile ...
rank:125,optimizer_step time: 4.869120121002197
rank:125, finish optimizer.step profile ...
rank:125, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:125, trace log has been written to txt...
rank:125, finish release GPU memory ...
rank:125, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 126, finish warm up ...
rank_id = 126, input_tensor_shapes: [(512, 1, 4096)]
rank:126,cuda fwd time: 7.354368209838867
rank:126, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425201893.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201893.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201894.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201895.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201896.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201897.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201898.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425201899.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 126, finish FWD profile ...
rank:126, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425201901.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425201904.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201906.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201909.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201911.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425201914.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201916.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425201919.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:126, finish BWD profile ...
rank:126,optimizer_step time: 4.881408214569092
rank:126, finish optimizer.step profile ...
rank:126, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:126, trace log has been written to txt...
rank:126, finish release GPU memory ...
rank:126, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 127, finish warm up ...
rank_id = 127, input_tensor_shapes: [(512, 1, 4096)]
rank:127,cuda fwd time: 7.361536026000977
rank:127, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425202129.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202130.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202131.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202132.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202133.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202134.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202135.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202136.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 127, finish FWD profile ...
rank:127, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425202138.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202141.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425202143.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202146.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202148.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202151.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425202153.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202156.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:127, finish BWD profile ...
rank:127,optimizer_step time: 4.900864124298096
rank:127, finish optimizer.step profile ...
rank:127, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:127, trace log has been written to txt...
rank:127, finish release GPU memory ...
rank:127, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 128, finish warm up ...
rank_id = 128, input_tensor_shapes: [(512, 1, 4096)]
rank:128,cuda fwd time: 7.314432144165039
rank:128, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425202348.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202349.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202350.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202351.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202352.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202352.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202353.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202354.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 128, finish FWD profile ...
rank:128, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425202357.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202359.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202362.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202365.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202367.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202369.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202372.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202375.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:128, finish BWD profile ...
rank:128,optimizer_step time: 4.860928058624268
rank:128, finish optimizer.step profile ...
rank:128, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:128, trace log has been written to txt...
rank:128, finish release GPU memory ...
rank:128, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 129, finish warm up ...
rank_id = 129, input_tensor_shapes: [(512, 1, 4096)]
rank:129,cuda fwd time: 7.311359882354736
rank:129, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425202567.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202568.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202569.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202570.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202571.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202571.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202572.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202573.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 129, finish FWD profile ...
rank:129, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425202575.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202578.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202581.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202583.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202586.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202588.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202591.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202593.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:129, finish BWD profile ...
rank:129,optimizer_step time: 4.846591949462891
rank:129, finish optimizer.step profile ...
rank:129, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:129, trace log has been written to txt...
rank:129, finish release GPU memory ...
rank:129, After memory release - Allocated: 910401024, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 130, finish warm up ...
rank_id = 130, input_tensor_shapes: [(512, 1, 4096)]
rank:130,cuda fwd time: 7.24889612197876
rank:130, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425202823.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202823.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202824.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202825.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202826.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202827.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202828.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425202829.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 130, finish FWD profile ...
rank:130, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425202831.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202834.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202836.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202839.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202841.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202844.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202846.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425202849.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:130, finish BWD profile ...
rank:130,optimizer_step time: 4.921343803405762
rank:130, finish optimizer.step profile ...
rank:130, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:130, trace log has been written to txt...
rank:130, finish release GPU memory ...
rank:130, After memory release - Allocated: 911187456, Reserved: 1451229184
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 131, finish warm up ...
rank_id = 131, input_tensor_shapes: [(512, 1, 4096)]
rank:131,cuda fwd time: 7.33081579208374
rank:131, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425203067.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203068.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203069.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203070.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203071.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203071.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203072.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203073.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 131, finish FWD profile ...
rank:131, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425203076.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203079.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203081.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425203084.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203086.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203089.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203091.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203094.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:131, finish BWD profile ...
rank:131,optimizer_step time: 4.914175987243652
rank:131, finish optimizer.step profile ...
rank:131, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:131, trace log has been written to txt...
rank:131, finish release GPU memory ...
rank:131, After memory release - Allocated: 910401024, Reserved: 1451229184
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 132, finish warm up ...
rank_id = 132, input_tensor_shapes: [(512, 1, 4096)]
rank:132,cuda fwd time: 7.335936069488525
rank:132, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425203309.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203310.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203311.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203312.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203313.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203314.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203315.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203315.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 132, finish FWD profile ...
rank:132, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425203318.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203321.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203323.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203326.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203328.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203331.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425203333.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203336.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:132, finish BWD profile ...
rank:132,optimizer_step time: 4.901887893676758
rank:132, finish optimizer.step profile ...
rank:132, Before memory release - Allocated: 2993584128, Reserved: 3091202048
rank:132, trace log has been written to txt...
rank:132, finish release GPU memory ...
rank:132, After memory release - Allocated: 911187456, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 133, finish warm up ...
rank_id = 133, input_tensor_shapes: [(512, 1, 4096)]
rank:133,cuda fwd time: 8.171520233154297
rank:133, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425203555.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203556.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203557.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203558.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203559.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203560.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203561.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203561.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 133, finish FWD profile ...
rank:133, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425203564.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203567.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203569.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203572.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203574.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203577.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425203579.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203582.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:133, finish BWD profile ...
rank:133,optimizer_step time: 4.965375900268555
rank:133, finish optimizer.step profile ...
rank:133, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:133, trace log has been written to txt...
rank:133, finish release GPU memory ...
rank:133, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 134, finish warm up ...
rank_id = 134, input_tensor_shapes: [(512, 1, 4096)]
rank:134,cuda fwd time: 7.328767776489258
rank:134, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425203778.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203779.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203780.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203781.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203782.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203782.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203783.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425203784.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 134, finish FWD profile ...
rank:134, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425203787.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203789.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203792.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203795.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203797.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203800.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203802.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425203805.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:134, finish BWD profile ...
rank:134,optimizer_step time: 4.888576030731201
rank:134, finish optimizer.step profile ...
rank:134, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:134, trace log has been written to txt...
rank:134, finish release GPU memory ...
rank:134, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 4): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 135, finish warm up ...
rank_id = 135, input_tensor_shapes: [(512, 1, 4096)]
rank:135,cuda fwd time: 7.4967041015625
rank:135, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425204006.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204007.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204008.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204009.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204010.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204011.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204012.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204012.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 135, finish FWD profile ...
rank:135, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425204015.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204018.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204020.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204023.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204025.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204028.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425204030.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204033.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:135, finish BWD profile ...
rank:135,optimizer_step time: 4.9203200340271
rank:135, finish optimizer.step profile ...
rank:135, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:135, trace log has been written to txt...
rank:135, finish release GPU memory ...
rank:135, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 136, finish warm up ...
rank_id = 136, input_tensor_shapes: [(512, 1, 4096)]
rank:136,cuda fwd time: 7.352320194244385
rank:136, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425204237.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204237.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204238.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204239.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204240.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204241.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204242.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204243.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 136, finish FWD profile ...
rank:136, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425204245.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204248.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204250.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204253.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204255.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204258.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204260.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204263.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:136, finish BWD profile ...
rank:136,optimizer_step time: 4.890624046325684
rank:136, finish optimizer.step profile ...
rank:136, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:136, trace log has been written to txt...
rank:136, finish release GPU memory ...
rank:136, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 137, finish warm up ...
rank_id = 137, input_tensor_shapes: [(512, 1, 4096)]
rank:137,cuda fwd time: 7.321599960327148
rank:137, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425204456.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425204457.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204458.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204458.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204459.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204460.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204461.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204462.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 137, finish FWD profile ...
rank:137, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425204464.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204467.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204469.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204472.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204474.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204477.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204479.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204482.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:137, finish BWD profile ...
rank:137,optimizer_step time: 4.855807781219482
rank:137, finish optimizer.step profile ...
rank:137, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:137, trace log has been written to txt...
rank:137, finish release GPU memory ...
rank:137, After memory release - Allocated: 910401024, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 138, finish warm up ...
rank_id = 138, input_tensor_shapes: [(512, 1, 4096)]
rank:138,cuda fwd time: 7.309311866760254
rank:138, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425204675.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204676.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204677.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204678.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204679.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204679.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204680.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425204681.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 138, finish FWD profile ...
rank:138, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425204683.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204686.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204689.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204691.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204694.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204696.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204699.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204701.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:138, finish BWD profile ...
rank:138,optimizer_step time: 4.896768093109131
rank:138, finish optimizer.step profile ...
rank:138, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:138, trace log has been written to txt...
rank:138, finish release GPU memory ...
rank:138, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 139, finish warm up ...
rank_id = 139, input_tensor_shapes: [(512, 1, 4096)]
rank:139,cuda fwd time: 7.99129581451416
rank:139, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425204943.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204944.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204945.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204946.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204947.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204948.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204949.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425204950.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 139, finish FWD profile ...
rank:139, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425204952.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204955.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204957.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204960.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204962.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204965.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425204967.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425204970.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:139, finish BWD profile ...
rank:139,optimizer_step time: 4.959231853485107
rank:139, finish optimizer.step profile ...
rank:139, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:139, trace log has been written to txt...
rank:139, finish release GPU memory ...
rank:139, After memory release - Allocated: 910401024, Reserved: 1507852288
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 140, finish warm up ...
rank_id = 140, input_tensor_shapes: [(512, 1, 4096)]
rank:140,cuda fwd time: 7.386112213134766
rank:140, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425205199.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205200.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205201.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205202.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205203.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205203.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205204.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205205.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 140, finish FWD profile ...
rank:140, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425205208.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205210.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205213.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205216.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205218.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205221.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425205223.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205225.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:140, finish BWD profile ...
rank:140,optimizer_step time: 4.9100799560546875
rank:140, finish optimizer.step profile ...
rank:140, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:140, trace log has been written to txt...
rank:140, finish release GPU memory ...
rank:140, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 141, finish warm up ...
rank_id = 141, input_tensor_shapes: [(512, 1, 4096)]
rank:141,cuda fwd time: 7.648255825042725
rank:141, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425205452.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205453.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205454.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205454.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205456.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205456.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205457.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205458.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 141, finish FWD profile ...
rank:141, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425205461.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205463.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205466.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205468.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205471.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205473.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205476.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205478.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:141, finish BWD profile ...
rank:141,optimizer_step time: 4.905983924865723
rank:141, finish optimizer.step profile ...
rank:141, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:141, trace log has been written to txt...
rank:141, finish release GPU memory ...
rank:141, After memory release - Allocated: 910401024, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 142, finish warm up ...
rank_id = 142, input_tensor_shapes: [(512, 1, 4096)]
rank:142,cuda fwd time: 7.269375801086426
rank:142, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425205673.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205674.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205675.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205676.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205677.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205678.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205679.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205679.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 142, finish FWD profile ...
rank:142, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425205682.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205685.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205687.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205690.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205692.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205695.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205697.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205700.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:142, finish BWD profile ...
rank:142,optimizer_step time: 4.874239921569824
rank:142, finish optimizer.step profile ...
rank:142, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:142, trace log has been written to txt...
rank:142, finish release GPU memory ...
rank:142, After memory release - Allocated: 911187456, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 143, finish warm up ...
rank_id = 143, input_tensor_shapes: [(512, 1, 4096)]
rank:143,cuda fwd time: 7.779327869415283
rank:143, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425205901.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205902.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205903.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205904.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205905.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205906.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205907.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425205907.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 143, finish FWD profile ...
rank:143, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425205910.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205913.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205915.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205918.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205920.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205923.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205925.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425205928.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:143, finish BWD profile ...
rank:143,optimizer_step time: 4.923391819000244
rank:143, finish optimizer.step profile ...
rank:143, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:143, trace log has been written to txt...
rank:143, finish release GPU memory ...
rank:143, After memory release - Allocated: 910401024, Reserved: 1472200704
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 144, finish warm up ...
rank_id = 144, input_tensor_shapes: [(512, 1, 4096)]
rank:144,cuda fwd time: 8.39577579498291
rank:144, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425206164.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206165.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206166.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206167.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206168.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206169.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206170.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206171.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 144, finish FWD profile ...
rank:144, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425206173.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206176.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206179.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206181.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206184.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425206186.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206189.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206191.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:144, finish BWD profile ...
rank:144,optimizer_step time: 4.978687763214111
rank:144, finish optimizer.step profile ...
rank:144, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:144, trace log has been written to txt...
rank:144, finish release GPU memory ...
rank:144, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 145, finish warm up ...
rank_id = 145, input_tensor_shapes: [(512, 1, 4096)]
rank:145,cuda fwd time: 7.283711910247803
rank:145, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425206429.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206429.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206430.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206431.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206432.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206433.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206434.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206435.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 145, finish FWD profile ...
rank:145, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.08,timestamp=2425206437.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206440.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206442.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206445.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425206447.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206450.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206452.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206455.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:145, finish BWD profile ...
rank:145,optimizer_step time: 4.919295787811279
rank:145, finish optimizer.step profile ...
rank:145, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:145, trace log has been written to txt...
rank:145, finish release GPU memory ...
rank:145, After memory release - Allocated: 910401024, Reserved: 1423966208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 146, finish warm up ...
rank_id = 146, input_tensor_shapes: [(512, 1, 4096)]
rank:146,cuda fwd time: 8.39475154876709
rank:146, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425206693.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206694.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206695.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206696.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206697.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206698.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206699.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206700.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 146, finish FWD profile ...
rank:146, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425206702.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206705.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206707.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206710.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206712.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206715.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425206717.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206720.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:146, finish BWD profile ...
rank:146,optimizer_step time: 4.9100799560546875
rank:146, finish optimizer.step profile ...
rank:146, Before memory release - Allocated: 2993584128, Reserved: 3091202048
rank:146, trace log has been written to txt...
rank:146, finish release GPU memory ...
rank:146, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 147, finish warm up ...
rank_id = 147, input_tensor_shapes: [(512, 1, 4096)]
rank:147,cuda fwd time: 7.965695858001709
rank:147, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425206938.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206939.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206940.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206941.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206942.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206943.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206944.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425206945.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 147, finish FWD profile ...
rank:147, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425206947.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206950.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206953.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206955.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206958.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425206960.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206963.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425206965.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:147, finish BWD profile ...
rank:147,optimizer_step time: 4.919295787811279
rank:147, finish optimizer.step profile ...
rank:147, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:147, trace log has been written to txt...
rank:147, finish release GPU memory ...
rank:147, After memory release - Allocated: 910401024, Reserved: 1484783616
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 148, finish warm up ...
rank_id = 148, input_tensor_shapes: [(512, 1, 4096)]
rank:148,cuda fwd time: 7.285759925842285
rank:148, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425207381.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207382.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207383.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207384.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207385.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207385.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207386.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207387.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 148, finish FWD profile ...
rank:148, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425207389.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207392.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207395.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207397.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207400.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207402.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207405.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207407.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:148, finish BWD profile ...
rank:148,optimizer_step time: 4.95308780670166
rank:148, finish optimizer.step profile ...
rank:148, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:148, trace log has been written to txt...
rank:148, finish release GPU memory ...
rank:148, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 149, finish warm up ...
rank_id = 149, input_tensor_shapes: [(512, 1, 4096)]
rank:149,cuda fwd time: 8.6364164352417
rank:149, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425207926.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425207927.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207928.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207929.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207930.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207931.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207932.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425207933.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 149, finish FWD profile ...
rank:149, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425207936.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207939.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207941.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207944.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425207946.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207949.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425207951.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425207954.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:149, finish BWD profile ...
rank:149,optimizer_step time: 4.942848205566406
rank:149, finish optimizer.step profile ...
rank:149, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:149, trace log has been written to txt...
rank:149, finish release GPU memory ...
rank:149, After memory release - Allocated: 910401024, Reserved: 1432354816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 150, finish warm up ...
rank_id = 150, input_tensor_shapes: [(512, 1, 4096)]
rank:150,cuda fwd time: 7.709695816040039
rank:150, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425208186.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208187.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208188.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208189.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208190.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208190.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208192.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208192.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 150, finish FWD profile ...
rank:150, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425208195.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208198.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208200.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208203.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208205.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208208.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208210.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208213.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:150, finish BWD profile ...
rank:150,optimizer_step time: 4.8752641677856445
rank:150, finish optimizer.step profile ...
rank:150, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:150, trace log has been written to txt...
rank:150, finish release GPU memory ...
rank:150, After memory release - Allocated: 911187456, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 151, finish warm up ...
rank_id = 151, input_tensor_shapes: [(512, 1, 4096)]
rank:151,cuda fwd time: 7.311359882354736
rank:151, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425208417.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208418.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208419.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208419.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208420.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208421.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208422.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208423.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 151, finish FWD profile ...
rank:151, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425208425.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208428.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208431.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208433.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208436.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208438.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208441.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208443.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:151, finish BWD profile ...
rank:151,optimizer_step time: 4.867072105407715
rank:151, finish optimizer.step profile ...
rank:151, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:151, trace log has been written to txt...
rank:151, finish release GPU memory ...
rank:151, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 152, finish warm up ...
rank_id = 152, input_tensor_shapes: [(512, 1, 4096)]
rank:152,cuda fwd time: 7.326720237731934
rank:152, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425208645.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208645.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208646.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208647.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208648.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208649.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208650.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208651.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 152, finish FWD profile ...
rank:152, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425208653.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208656.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208658.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208661.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208663.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208666.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208668.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208671.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:152, finish BWD profile ...
rank:152,optimizer_step time: 4.881408214569092
rank:152, finish optimizer.step profile ...
rank:152, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:152, trace log has been written to txt...
rank:152, finish release GPU memory ...
rank:152, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 153, finish warm up ...
rank_id = 153, input_tensor_shapes: [(512, 1, 4096)]
rank:153,cuda fwd time: 7.7711358070373535
rank:153, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425208891.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208892.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208893.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208893.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208895.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208895.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208896.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425208897.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 153, finish FWD profile ...
rank:153, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425208900.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208902.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425208905.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208908.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208910.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208913.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425208915.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425208918.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:153, finish BWD profile ...
rank:153,optimizer_step time: 4.964352130889893
rank:153, finish optimizer.step profile ...
rank:153, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:153, trace log has been written to txt...
rank:153, finish release GPU memory ...
rank:153, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 154, finish warm up ...
rank_id = 154, input_tensor_shapes: [(512, 1, 4096)]
rank:154,cuda fwd time: 7.445504188537598
rank:154, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425209140.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209141.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209142.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209143.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209144.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209145.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209146.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209146.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 154, finish FWD profile ...
rank:154, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425209149.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209152.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209154.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209157.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209159.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209162.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425209164.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209167.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:154, finish BWD profile ...
rank:154,optimizer_step time: 4.9203200340271
rank:154, finish optimizer.step profile ...
rank:154, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:154, trace log has been written to txt...
rank:154, finish release GPU memory ...
rank:154, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 155, finish warm up ...
rank_id = 155, input_tensor_shapes: [(512, 1, 4096)]
rank:155,cuda fwd time: 7.305215835571289
rank:155, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425209365.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209365.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209366.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209367.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209368.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209369.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209370.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209371.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 155, finish FWD profile ...
rank:155, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425209373.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209376.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209378.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209381.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209383.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209386.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209388.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209391.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:155, finish BWD profile ...
rank:155,optimizer_step time: 4.863999843597412
rank:155, finish optimizer.step profile ...
rank:155, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:155, trace log has been written to txt...
rank:155, finish release GPU memory ...
rank:155, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 156, finish warm up ...
rank_id = 156, input_tensor_shapes: [(512, 1, 4096)]
rank:156,cuda fwd time: 7.24070405960083
rank:156, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425209590.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209590.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209591.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209592.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209593.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209594.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209595.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209596.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 156, finish FWD profile ...
rank:156, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425209598.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209601.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209603.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209606.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209608.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209611.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209613.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209616.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:156, finish BWD profile ...
rank:156,optimizer_step time: 4.887551784515381
rank:156, finish optimizer.step profile ...
rank:156, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:156, trace log has been written to txt...
rank:156, finish release GPU memory ...
rank:156, After memory release - Allocated: 911187456, Reserved: 1484783616
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 157, finish warm up ...
rank_id = 157, input_tensor_shapes: [(512, 1, 4096)]
rank:157,cuda fwd time: 7.773183822631836
rank:157, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425209832.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209833.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209834.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209835.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209836.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209837.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209838.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425209838.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 157, finish FWD profile ...
rank:157, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425209841.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209844.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209846.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209849.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209851.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209854.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209856.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425209859.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:157, finish BWD profile ...
rank:157,optimizer_step time: 4.8752641677856445
rank:157, finish optimizer.step profile ...
rank:157, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:157, trace log has been written to txt...
rank:157, finish release GPU memory ...
rank:157, After memory release - Allocated: 910401024, Reserved: 1495269376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 158, finish warm up ...
rank_id = 158, input_tensor_shapes: [(512, 1, 4096)]
rank:158,cuda fwd time: 7.312384128570557
rank:158, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425210086.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210087.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210088.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210089.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210090.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210091.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210092.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210092.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 158, finish FWD profile ...
rank:158, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425210095.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210098.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210100.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210103.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210105.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210108.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425210110.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210113.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:158, finish BWD profile ...
rank:158,optimizer_step time: 4.940800189971924
rank:158, finish optimizer.step profile ...
rank:158, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:158, trace log has been written to txt...
rank:158, finish release GPU memory ...
rank:158, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 159, finish warm up ...
rank_id = 159, input_tensor_shapes: [(512, 1, 4096)]
rank:159,cuda fwd time: 7.314432144165039
rank:159, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425210329.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210330.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210331.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210332.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210333.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210333.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210334.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210335.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 159, finish FWD profile ...
rank:159, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425210338.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210340.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210343.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210346.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210348.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210350.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210353.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210355.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:159, finish BWD profile ...
rank:159,optimizer_step time: 4.881408214569092
rank:159, finish optimizer.step profile ...
rank:159, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:159, trace log has been written to txt...
rank:159, finish release GPU memory ...
rank:159, After memory release - Allocated: 910401024, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 160, finish warm up ...
rank_id = 160, input_tensor_shapes: [(512, 1, 4096)]
rank:160,cuda fwd time: 7.279615879058838
rank:160, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425210557.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210558.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210559.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210560.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210561.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210562.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210563.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210563.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 160, finish FWD profile ...
rank:160, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425210566.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210569.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210571.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210574.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210576.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210579.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210581.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210584.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:160, finish BWD profile ...
rank:160,optimizer_step time: 4.931583881378174
rank:160, finish optimizer.step profile ...
rank:160, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:160, trace log has been written to txt...
rank:160, finish release GPU memory ...
rank:160, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 161, finish warm up ...
rank_id = 161, input_tensor_shapes: [(512, 1, 4096)]
rank:161,cuda fwd time: 7.354368209838867
rank:161, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425210785.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210786.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210787.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210788.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210789.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210790.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210791.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425210791.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 161, finish FWD profile ...
rank:161, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425210794.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210797.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210799.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210802.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210804.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210807.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210809.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425210812.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:161, finish BWD profile ...
rank:161,optimizer_step time: 4.930560111999512
rank:161, finish optimizer.step profile ...
rank:161, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:161, trace log has been written to txt...
rank:161, finish release GPU memory ...
rank:161, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 162, finish warm up ...
rank_id = 162, input_tensor_shapes: [(512, 1, 4096)]
rank:162,cuda fwd time: 7.685120105743408
rank:162, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425211018.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211019.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211020.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211021.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211022.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211022.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211023.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211024.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 162, finish FWD profile ...
rank:162, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425211027.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211029.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211032.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211035.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211037.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211040.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211042.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211044.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:162, finish BWD profile ...
rank:162,optimizer_step time: 4.883456230163574
rank:162, finish optimizer.step profile ...
rank:162, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:162, trace log has been written to txt...
rank:162, finish release GPU memory ...
rank:162, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 163, finish warm up ...
rank_id = 163, input_tensor_shapes: [(512, 1, 4096)]
rank:163,cuda fwd time: 7.3072638511657715
rank:163, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425211244.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211245.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211246.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211246.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211247.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211248.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211249.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211250.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 163, finish FWD profile ...
rank:163, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425211252.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425211255.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211258.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211260.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211263.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211265.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211268.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211270.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:163, finish BWD profile ...
rank:163,optimizer_step time: 4.881408214569092
rank:163, finish optimizer.step profile ...
rank:163, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:163, trace log has been written to txt...
rank:163, finish release GPU memory ...
rank:163, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 164, finish warm up ...
rank_id = 164, input_tensor_shapes: [(512, 1, 4096)]
rank:164,cuda fwd time: 7.271423816680908
rank:164, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425211469.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211470.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211471.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211472.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211473.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211473.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211474.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211475.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 164, finish FWD profile ...
rank:164, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425211478.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211480.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211483.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211486.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211488.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211490.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211493.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211495.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:164, finish BWD profile ...
rank:164,optimizer_step time: 4.887551784515381
rank:164, finish optimizer.step profile ...
rank:164, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:164, trace log has been written to txt...
rank:164, finish release GPU memory ...
rank:164, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 165, finish warm up ...
rank_id = 165, input_tensor_shapes: [(512, 1, 4096)]
rank:165,cuda fwd time: 7.377920150756836
rank:165, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425211707.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211708.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211709.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211710.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211711.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211711.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211713.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211713.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 165, finish FWD profile ...
rank:165, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425211716.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211719.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211721.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211724.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211726.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211729.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211731.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211734.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:165, finish BWD profile ...
rank:165,optimizer_step time: 4.90393590927124
rank:165, finish optimizer.step profile ...
rank:165, Before memory release - Allocated: 2993584128, Reserved: 3091202048
rank:165, trace log has been written to txt...
rank:165, finish release GPU memory ...
rank:165, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 166, finish warm up ...
rank_id = 166, input_tensor_shapes: [(512, 1, 4096)]
rank:166,cuda fwd time: 7.251967906951904
rank:166, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425211937.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211938.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211939.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211940.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211941.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211941.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425211942.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425211943.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 166, finish FWD profile ...
rank:166, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425211946.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211948.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211951.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211954.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425211956.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211958.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211961.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425211963.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:166, finish BWD profile ...
rank:166,optimizer_step time: 4.908031940460205
rank:166, finish optimizer.step profile ...
rank:166, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:166, trace log has been written to txt...
rank:166, finish release GPU memory ...
rank:166, After memory release - Allocated: 911187456, Reserved: 1484783616
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 5): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 167, finish warm up ...
rank_id = 167, input_tensor_shapes: [(512, 1, 4096)]
rank:167,cuda fwd time: 7.385087966918945
rank:167, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425212186.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212186.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212188.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212188.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212189.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212190.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212191.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212192.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 167, finish FWD profile ...
rank:167, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425212194.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212197.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212199.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212202.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212204.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425212207.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212210.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212212.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:167, finish BWD profile ...
rank:167,optimizer_step time: 4.946944236755371
rank:167, finish optimizer.step profile ...
rank:167, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:167, trace log has been written to txt...
rank:167, finish release GPU memory ...
rank:167, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 168, finish warm up ...
rank_id = 168, input_tensor_shapes: [(512, 1, 4096)]
rank:168,cuda fwd time: 7.355391979217529
rank:168, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425212416.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212417.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212418.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212418.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212420.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212420.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212421.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212422.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 168, finish FWD profile ...
rank:168, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425212424.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212427.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212430.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212432.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212435.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212437.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212440.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212442.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:168, finish BWD profile ...
rank:168,optimizer_step time: 6.1091837882995605
rank:168, finish optimizer.step profile ...
rank:168, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:168, trace log has been written to txt...
rank:168, finish release GPU memory ...
rank:168, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 169, finish warm up ...
rank_id = 169, input_tensor_shapes: [(512, 1, 4096)]
rank:169,cuda fwd time: 7.311359882354736
rank:169, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425212639.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212640.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212641.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212641.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212643.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212643.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212644.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212645.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 169, finish FWD profile ...
rank:169, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425212647.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212650.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212653.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212655.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212658.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212660.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212663.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212665.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:169, finish BWD profile ...
rank:169,optimizer_step time: 4.90393590927124
rank:169, finish optimizer.step profile ...
rank:169, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:169, trace log has been written to txt...
rank:169, finish release GPU memory ...
rank:169, After memory release - Allocated: 910401024, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 170, finish warm up ...
rank_id = 170, input_tensor_shapes: [(512, 1, 4096)]
rank:170,cuda fwd time: 7.314432144165039
rank:170, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425212858.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212859.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212860.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212861.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212862.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212863.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212864.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425212864.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 170, finish FWD profile ...
rank:170, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425212867.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212870.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212872.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212875.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425212877.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212880.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212882.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425212885.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:170, finish BWD profile ...
rank:170,optimizer_step time: 4.900864124298096
rank:170, finish optimizer.step profile ...
rank:170, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:170, trace log has been written to txt...
rank:170, finish release GPU memory ...
rank:170, After memory release - Allocated: 911187456, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 171, finish warm up ...
rank_id = 171, input_tensor_shapes: [(512, 1, 4096)]
rank:171,cuda fwd time: 7.245823860168457
rank:171, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425213109.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213110.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213111.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213111.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213112.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213113.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213114.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213115.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 171, finish FWD profile ...
rank:171, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425213117.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213120.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213122.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213125.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213127.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213130.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425213132.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213135.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:171, finish BWD profile ...
rank:171,optimizer_step time: 4.908031940460205
rank:171, finish optimizer.step profile ...
rank:171, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:171, trace log has been written to txt...
rank:171, finish release GPU memory ...
rank:171, After memory release - Allocated: 910401024, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 172, finish warm up ...
rank_id = 172, input_tensor_shapes: [(512, 1, 4096)]
rank:172,cuda fwd time: 7.746560096740723
rank:172, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425213359.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213360.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213361.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213362.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213363.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213364.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213365.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213366.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 172, finish FWD profile ...
rank:172, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425213368.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213371.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213373.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213376.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213378.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213381.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213383.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213386.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:172, finish BWD profile ...
rank:172,optimizer_step time: 4.956160068511963
rank:172, finish optimizer.step profile ...
rank:172, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:172, trace log has been written to txt...
rank:172, finish release GPU memory ...
rank:172, After memory release - Allocated: 911187456, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 173, finish warm up ...
rank_id = 173, input_tensor_shapes: [(512, 1, 4096)]
rank:173,cuda fwd time: 7.59500789642334
rank:173, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425213605.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213606.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213607.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213608.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213609.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213609.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213610.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213611.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 173, finish FWD profile ...
rank:173, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425213614.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213616.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213619.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213622.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213624.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213627.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213629.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213632.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:173, finish BWD profile ...
rank:173,optimizer_step time: 4.891647815704346
rank:173, finish optimizer.step profile ...
rank:173, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:173, trace log has been written to txt...
rank:173, finish release GPU memory ...
rank:173, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 174, finish warm up ...
rank_id = 174, input_tensor_shapes: [(512, 1, 4096)]
rank:174,cuda fwd time: 7.823359966278076
rank:174, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425213841.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213841.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213842.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213843.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213844.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213845.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213846.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425213847.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 174, finish FWD profile ...
rank:174, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425213849.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213852.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213855.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213857.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213860.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213862.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425213865.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425213867.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:174, finish BWD profile ...
rank:174,optimizer_step time: 4.916224002838135
rank:174, finish optimizer.step profile ...
rank:174, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:174, trace log has been written to txt...
rank:174, finish release GPU memory ...
rank:174, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 175, finish warm up ...
rank_id = 175, input_tensor_shapes: [(512, 1, 4096)]
rank:175,cuda fwd time: 7.372799873352051
rank:175, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425214075.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214075.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214077.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214077.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214078.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214079.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214080.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214081.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 175, finish FWD profile ...
rank:175, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.08,timestamp=2425214083.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214086.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214088.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425214091.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214093.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214096.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214098.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214101.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:175, finish BWD profile ...
rank:175,optimizer_step time: 4.898816108703613
rank:175, finish optimizer.step profile ...
rank:175, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:175, trace log has been written to txt...
rank:175, finish release GPU memory ...
rank:175, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 176, finish warm up ...
rank_id = 176, input_tensor_shapes: [(512, 1, 4096)]
rank:176,cuda fwd time: 7.3461761474609375
rank:176, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425214326.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214327.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214328.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214329.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214330.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214330.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214331.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425214332.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 176, finish FWD profile ...
rank:176, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.08,timestamp=2425214335.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214338.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214340.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214343.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214345.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214348.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214350.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214353.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:176, finish BWD profile ...
rank:176,optimizer_step time: 4.907008171081543
rank:176, finish optimizer.step profile ...
rank:176, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:176, trace log has been written to txt...
rank:176, finish release GPU memory ...
rank:176, After memory release - Allocated: 911187456, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 177, finish warm up ...
rank_id = 177, input_tensor_shapes: [(512, 1, 4096)]
rank:177,cuda fwd time: 7.352320194244385
rank:177, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425214566.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214567.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214568.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214568.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214569.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214570.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214571.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214572.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 177, finish FWD profile ...
rank:177, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425214574.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214577.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214580.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214582.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425214585.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214587.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425214589.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214592.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:177, finish BWD profile ...
rank:177,optimizer_step time: 4.921343803405762
rank:177, finish optimizer.step profile ...
rank:177, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:177, trace log has been written to txt...
rank:177, finish release GPU memory ...
rank:177, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 178, finish warm up ...
rank_id = 178, input_tensor_shapes: [(512, 1, 4096)]
rank:178,cuda fwd time: 7.205887794494629
rank:178, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425214824.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214825.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214826.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214827.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214828.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214828.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214829.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425214830.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 178, finish FWD profile ...
rank:178, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425214832.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214835.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214838.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214840.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425214843.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214845.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425214848.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425214850.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:178, finish BWD profile ...
rank:178,optimizer_step time: 4.932608127593994
rank:178, finish optimizer.step profile ...
rank:178, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:178, trace log has been written to txt...
rank:178, finish release GPU memory ...
rank:178, After memory release - Allocated: 911187456, Reserved: 1507852288
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 179, finish warm up ...
rank_id = 179, input_tensor_shapes: [(512, 1, 4096)]
rank:179,cuda fwd time: 7.244800090789795
rank:179, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425215080.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215080.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215082.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215082.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215083.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215084.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215085.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215086.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 179, finish FWD profile ...
rank:179, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425215088.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215091.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215093.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215096.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215098.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215101.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215103.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215106.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:179, finish BWD profile ...
rank:179,optimizer_step time: 4.933631896972656
rank:179, finish optimizer.step profile ...
rank:179, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:179, trace log has been written to txt...
rank:179, finish release GPU memory ...
rank:179, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 180, finish warm up ...
rank_id = 180, input_tensor_shapes: [(512, 1, 4096)]
rank:180,cuda fwd time: 7.326720237731934
rank:180, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425215319.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215320.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215321.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215322.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215323.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215324.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215325.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215325.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 180, finish FWD profile ...
rank:180, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425215328.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215331.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215333.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215336.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215338.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215341.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425215343.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215346.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:180, finish BWD profile ...
rank:180,optimizer_step time: 4.914175987243652
rank:180, finish optimizer.step profile ...
rank:180, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:180, trace log has been written to txt...
rank:180, finish release GPU memory ...
rank:180, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 181, finish warm up ...
rank_id = 181, input_tensor_shapes: [(512, 1, 4096)]
rank:181,cuda fwd time: 7.28166389465332
rank:181, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425215543.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215544.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215545.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215546.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215547.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215548.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215549.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215549.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 181, finish FWD profile ...
rank:181, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425215552.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215555.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215557.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215560.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425215562.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215565.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425215567.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215570.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:181, finish BWD profile ...
rank:181,optimizer_step time: 4.90393590927124
rank:181, finish optimizer.step profile ...
rank:181, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:181, trace log has been written to txt...
rank:181, finish release GPU memory ...
rank:181, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 182, finish warm up ...
rank_id = 182, input_tensor_shapes: [(512, 1, 4096)]
rank:182,cuda fwd time: 7.532544136047363
rank:182, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425215788.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215789.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215790.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215791.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215792.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215792.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215793.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425215794.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 182, finish FWD profile ...
rank:182, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425215797.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215800.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215802.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215805.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215807.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215810.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215812.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425215815.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:182, finish BWD profile ...
rank:182,optimizer_step time: 4.931583881378174
rank:182, finish optimizer.step profile ...
rank:182, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:182, trace log has been written to txt...
rank:182, finish release GPU memory ...
rank:182, After memory release - Allocated: 911187456, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 183, finish warm up ...
rank_id = 183, input_tensor_shapes: [(512, 1, 4096)]
rank:183,cuda fwd time: 7.788544178009033
rank:183, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425216033.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216034.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216035.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216036.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216037.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216038.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216039.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216039.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 183, finish FWD profile ...
rank:183, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425216042.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216045.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425216047.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216050.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216052.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216055.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425216057.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216060.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:183, finish BWD profile ...
rank:183,optimizer_step time: 4.911104202270508
rank:183, finish optimizer.step profile ...
rank:183, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:183, trace log has been written to txt...
rank:183, finish release GPU memory ...
rank:183, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 184, finish warm up ...
rank_id = 184, input_tensor_shapes: [(512, 1, 4096)]
rank:184,cuda fwd time: 7.249919891357422
rank:184, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425216263.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216264.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216265.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216266.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216267.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216267.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216268.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425216269.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 184, finish FWD profile ...
rank:184, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425216272.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216274.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216277.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216280.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216282.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216284.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216287.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216289.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:184, finish BWD profile ...
rank:184,optimizer_step time: 4.878335952758789
rank:184, finish optimizer.step profile ...
rank:184, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:184, trace log has been written to txt...
rank:184, finish release GPU memory ...
rank:184, After memory release - Allocated: 911187456, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 185, finish warm up ...
rank_id = 185, input_tensor_shapes: [(512, 1, 4096)]
rank:185,cuda fwd time: 8.176639556884766
rank:185, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425216523.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216524.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216525.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216526.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216527.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216528.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216529.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216530.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 185, finish FWD profile ...
rank:185, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425216533.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216535.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216538.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216540.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425216543.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216545.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216548.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216550.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:185, finish BWD profile ...
rank:185,optimizer_step time: 4.9203200340271
rank:185, finish optimizer.step profile ...
rank:185, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:185, trace log has been written to txt...
rank:185, finish release GPU memory ...
rank:185, After memory release - Allocated: 910401024, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 186, finish warm up ...
rank_id = 186, input_tensor_shapes: [(512, 1, 4096)]
rank:186,cuda fwd time: 7.570432186126709
rank:186, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425216760.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216761.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216762.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216763.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216764.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216764.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216765.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216766.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 186, finish FWD profile ...
rank:186, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425216769.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216772.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216774.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216777.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425216779.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216782.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216784.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216787.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:186, finish BWD profile ...
rank:186,optimizer_step time: 4.9049601554870605
rank:186, finish optimizer.step profile ...
rank:186, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:186, trace log has been written to txt...
rank:186, finish release GPU memory ...
rank:186, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 187, finish warm up ...
rank_id = 187, input_tensor_shapes: [(512, 1, 4096)]
rank:187,cuda fwd time: 7.237631797790527
rank:187, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425216982.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216983.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216984.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216985.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216986.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216986.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216987.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425216988.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 187, finish FWD profile ...
rank:187, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425216990.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216993.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216996.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425216998.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217001.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217003.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217006.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217008.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:187, finish BWD profile ...
rank:187,optimizer_step time: 4.889599800109863
rank:187, finish optimizer.step profile ...
rank:187, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:187, trace log has been written to txt...
rank:187, finish release GPU memory ...
rank:187, After memory release - Allocated: 910401024, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 188, finish warm up ...
rank_id = 188, input_tensor_shapes: [(512, 1, 4096)]
rank:188,cuda fwd time: 7.303167819976807
rank:188, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425217218.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217218.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217220.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217220.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217221.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217222.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217223.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425217224.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 188, finish FWD profile ...
rank:188, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425217226.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217229.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217231.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217234.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217236.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217239.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217241.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217244.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:188, finish BWD profile ...
rank:188,optimizer_step time: 4.914175987243652
rank:188, finish optimizer.step profile ...
rank:188, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:188, trace log has been written to txt...
rank:188, finish release GPU memory ...
rank:188, After memory release - Allocated: 911187456, Reserved: 1474297856
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 189, finish warm up ...
rank_id = 189, input_tensor_shapes: [(512, 1, 4096)]
rank:189,cuda fwd time: 7.291903972625732
rank:189, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425217451.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217452.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217453.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217454.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217455.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217456.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217457.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217457.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 189, finish FWD profile ...
rank:189, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425217460.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217463.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217465.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217468.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425217470.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217473.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425217475.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217478.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:189, finish BWD profile ...
rank:189,optimizer_step time: 4.893695831298828
rank:189, finish optimizer.step profile ...
rank:189, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:189, trace log has been written to txt...
rank:189, finish release GPU memory ...
rank:189, After memory release - Allocated: 910401024, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 190, finish warm up ...
rank_id = 190, input_tensor_shapes: [(512, 1, 4096)]
rank:190,cuda fwd time: 7.274496078491211
rank:190, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425217677.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217678.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217679.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217680.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217681.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217682.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217683.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217683.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 190, finish FWD profile ...
rank:190, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425217686.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217689.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217691.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217694.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217696.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217699.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425217701.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217704.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:190, finish BWD profile ...
rank:190,optimizer_step time: 4.897791862487793
rank:190, finish optimizer.step profile ...
rank:190, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:190, trace log has been written to txt...
rank:190, finish release GPU memory ...
rank:190, After memory release - Allocated: 911187456, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 191, finish warm up ...
rank_id = 191, input_tensor_shapes: [(512, 1, 4096)]
rank:191,cuda fwd time: 8.48691177368164
rank:191, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425217936.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217937.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217938.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217939.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217940.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217941.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217942.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425217943.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 191, finish FWD profile ...
rank:191, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425217945.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217948.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217951.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217953.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217956.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217958.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425217961.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425217963.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:191, finish BWD profile ...
rank:191,optimizer_step time: 4.933631896972656
rank:191, finish optimizer.step profile ...
rank:191, Before memory release - Allocated: 2993584128, Reserved: 3091202048
rank:191, trace log has been written to txt...
rank:191, finish release GPU memory ...
rank:191, After memory release - Allocated: 910401024, Reserved: 1451229184
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 192, finish warm up ...
rank_id = 192, input_tensor_shapes: [(512, 1, 4096)]
rank:192,cuda fwd time: 8.028160095214844
rank:192, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425218214.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218215.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218216.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218216.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218218.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218218.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218220.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218220.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 192, finish FWD profile ...
rank:192, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425218223.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218226.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218228.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218231.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218233.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218236.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218238.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218241.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:192, finish BWD profile ...
rank:192,optimizer_step time: 4.948991775512695
rank:192, finish optimizer.step profile ...
rank:192, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:192, trace log has been written to txt...
rank:192, finish release GPU memory ...
rank:192, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 193, finish warm up ...
rank_id = 193, input_tensor_shapes: [(512, 1, 4096)]
rank:193,cuda fwd time: 7.267327785491943
rank:193, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425218443.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218444.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218445.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218445.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218446.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218447.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218448.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218449.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 193, finish FWD profile ...
rank:193, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425218451.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218454.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218456.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218459.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218461.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218464.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425218466.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218469.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:193, finish BWD profile ...
rank:193,optimizer_step time: 4.91212797164917
rank:193, finish optimizer.step profile ...
rank:193, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:193, trace log has been written to txt...
rank:193, finish release GPU memory ...
rank:193, After memory release - Allocated: 910401024, Reserved: 1423966208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 194, finish warm up ...
rank_id = 194, input_tensor_shapes: [(512, 1, 4096)]
rank:194,cuda fwd time: 7.24889612197876
rank:194, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425218672.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218673.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218674.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218675.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218676.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218676.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218677.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218678.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 194, finish FWD profile ...
rank:194, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425218680.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218683.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218686.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218688.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218691.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218693.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218696.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218698.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:194, finish BWD profile ...
rank:194,optimizer_step time: 4.879360198974609
rank:194, finish optimizer.step profile ...
rank:194, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:194, trace log has been written to txt...
rank:194, finish release GPU memory ...
rank:194, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 195, finish warm up ...
rank_id = 195, input_tensor_shapes: [(512, 1, 4096)]
rank:195,cuda fwd time: 7.305215835571289
rank:195, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425218895.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218896.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218897.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218898.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218899.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218900.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425218901.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425218901.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 195, finish FWD profile ...
rank:195, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425218904.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218907.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218909.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218912.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425218914.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218917.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425218919.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425218922.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:195, finish BWD profile ...
rank:195,optimizer_step time: 4.888576030731201
rank:195, finish optimizer.step profile ...
rank:195, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:195, trace log has been written to txt...
rank:195, finish release GPU memory ...
rank:195, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 196, finish warm up ...
rank_id = 196, input_tensor_shapes: [(512, 1, 4096)]
rank:196,cuda fwd time: 7.502848148345947
rank:196, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425219146.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219146.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219147.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219148.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219149.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219150.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219151.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219152.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 196, finish FWD profile ...
rank:196, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425219154.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219157.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219159.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219162.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219164.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219167.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219169.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219172.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:196, finish BWD profile ...
rank:196,optimizer_step time: 4.91315221786499
rank:196, finish optimizer.step profile ...
rank:196, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:196, trace log has been written to txt...
rank:196, finish release GPU memory ...
rank:196, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 197, finish warm up ...
rank_id = 197, input_tensor_shapes: [(512, 1, 4096)]
rank:197,cuda fwd time: 7.293951988220215
rank:197, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425219367.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219368.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219369.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219370.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219371.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425219371.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219372.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425219373.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 197, finish FWD profile ...
rank:197, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425219376.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219378.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219381.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219384.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219386.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219388.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219391.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219394.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:197, finish BWD profile ...
rank:197,optimizer_step time: 4.900864124298096
rank:197, finish optimizer.step profile ...
rank:197, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:197, trace log has been written to txt...
rank:197, finish release GPU memory ...
rank:197, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 198, finish warm up ...
rank_id = 198, input_tensor_shapes: [(512, 1, 4096)]
rank:198,cuda fwd time: 7.325695991516113
rank:198, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425219604.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219605.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219606.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219606.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219608.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219608.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219609.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219610.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 198, finish FWD profile ...
rank:198, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425219612.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219615.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219618.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219620.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219623.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219625.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425219628.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219630.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:198, finish BWD profile ...
rank:198,optimizer_step time: 4.902912139892578
rank:198, finish optimizer.step profile ...
rank:198, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:198, trace log has been written to txt...
rank:198, finish release GPU memory ...
rank:198, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 6): 101270272
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 199, finish warm up ...
rank_id = 199, input_tensor_shapes: [(512, 1, 4096)]
rank:199,cuda fwd time: 7.392255783081055
rank:199, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425219821.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219822.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219823.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219824.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219825.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219826.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219827.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425219827.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 199, finish FWD profile ...
rank:199, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425219830.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219833.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219835.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219838.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425219840.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219843.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425219845.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425219848.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:199, finish BWD profile ...
rank:199,optimizer_step time: 4.874239921569824
rank:199, finish optimizer.step profile ...
rank:199, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:199, trace log has been written to txt...
rank:199, finish release GPU memory ...
rank:199, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 200, finish warm up ...
rank_id = 200, input_tensor_shapes: [(512, 1, 4096)]
rank:200,cuda fwd time: 7.376895904541016
rank:200, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425220043.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220044.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220045.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220046.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220047.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220048.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220049.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220049.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 200, finish FWD profile ...
rank:200, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425220052.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220055.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220057.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220060.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220062.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220065.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425220067.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220070.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:200, finish BWD profile ...
rank:200,optimizer_step time: 4.897791862487793
rank:200, finish optimizer.step profile ...
rank:200, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:200, trace log has been written to txt...
rank:200, finish release GPU memory ...
rank:200, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 201, finish warm up ...
rank_id = 201, input_tensor_shapes: [(512, 1, 4096)]
rank:201,cuda fwd time: 7.367680072784424
rank:201, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425220283.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220284.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220285.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220285.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220286.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220287.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220288.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425220289.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 201, finish FWD profile ...
rank:201, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425220291.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220294.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220297.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425220299.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425220302.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425220304.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425220306.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220309.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:201, finish BWD profile ...
rank:201,optimizer_step time: 4.878335952758789
rank:201, finish optimizer.step profile ...
rank:201, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:201, trace log has been written to txt...
rank:201, finish release GPU memory ...
rank:201, After memory release - Allocated: 910401024, Reserved: 1507852288
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 202, finish warm up ...
rank_id = 202, input_tensor_shapes: [(512, 1, 4096)]
rank:202,cuda fwd time: 7.838719844818115
rank:202, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425220505.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220506.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220507.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220508.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220509.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220510.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220511.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220512.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 202, finish FWD profile ...
rank:202, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425220514.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220517.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220519.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220522.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220525.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220527.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425220530.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220532.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:202, finish BWD profile ...
rank:202,optimizer_step time: 4.919295787811279
rank:202, finish optimizer.step profile ...
rank:202, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:202, trace log has been written to txt...
rank:202, finish release GPU memory ...
rank:202, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 203, finish warm up ...
rank_id = 203, input_tensor_shapes: [(512, 1, 4096)]
rank:203,cuda fwd time: 7.314432144165039
rank:203, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425220730.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220731.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220732.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220732.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220734.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220734.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220735.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220736.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 203, finish FWD profile ...
rank:203, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425220738.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220741.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220744.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220746.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220749.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220751.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220754.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220756.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:203, finish BWD profile ...
rank:203,optimizer_step time: 4.893695831298828
rank:203, finish optimizer.step profile ...
rank:203, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:203, trace log has been written to txt...
rank:203, finish release GPU memory ...
rank:203, After memory release - Allocated: 910401024, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 204, finish warm up ...
rank_id = 204, input_tensor_shapes: [(512, 1, 4096)]
rank:204,cuda fwd time: 8.168448448181152
rank:204, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425220986.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220987.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220988.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220989.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220990.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220991.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220992.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425220993.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 204, finish FWD profile ...
rank:204, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425220995.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425220998.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221001.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221003.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221006.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221008.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221011.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221013.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:204, finish BWD profile ...
rank:204,optimizer_step time: 4.929535865783691
rank:204, finish optimizer.step profile ...
rank:204, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:204, trace log has been written to txt...
rank:204, finish release GPU memory ...
rank:204, After memory release - Allocated: 911187456, Reserved: 1430257664
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 205, finish warm up ...
rank_id = 205, input_tensor_shapes: [(512, 1, 4096)]
rank:205,cuda fwd time: 7.250944137573242
rank:205, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425221214.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221215.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221216.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221217.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221218.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221219.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221220.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221220.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 205, finish FWD profile ...
rank:205, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425221223.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221226.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221228.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221231.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221233.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221236.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221238.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221241.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:205, finish BWD profile ...
rank:205,optimizer_step time: 4.881408214569092
rank:205, finish optimizer.step profile ...
rank:205, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:205, trace log has been written to txt...
rank:205, finish release GPU memory ...
rank:205, After memory release - Allocated: 910401024, Reserved: 1472200704
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 206, finish warm up ...
rank_id = 206, input_tensor_shapes: [(512, 1, 4096)]
rank:206,cuda fwd time: 7.33081579208374
rank:206, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425221435.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221435.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221436.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221437.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221438.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221439.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221440.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221441.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 206, finish FWD profile ...
rank:206, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425221443.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221446.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221448.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221451.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221453.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221456.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221458.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221461.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:206, finish BWD profile ...
rank:206,optimizer_step time: 4.876287937164307
rank:206, finish optimizer.step profile ...
rank:206, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:206, trace log has been written to txt...
rank:206, finish release GPU memory ...
rank:206, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 207, finish warm up ...
rank_id = 207, input_tensor_shapes: [(512, 1, 4096)]
rank:207,cuda fwd time: 7.4997758865356445
rank:207, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425221679.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221679.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221680.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221681.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221682.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221683.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221684.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221685.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 207, finish FWD profile ...
rank:207, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425221687.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221690.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221692.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221695.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221697.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221700.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221702.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221705.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:207, finish BWD profile ...
rank:207,optimizer_step time: 4.894720077514648
rank:207, finish optimizer.step profile ...
rank:207, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:207, trace log has been written to txt...
rank:207, finish release GPU memory ...
rank:207, After memory release - Allocated: 910401024, Reserved: 1423966208
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 208, finish warm up ...
rank_id = 208, input_tensor_shapes: [(512, 1, 4096)]
rank:208,cuda fwd time: 7.352320194244385
rank:208, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425221898.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221899.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221900.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221901.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221902.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221902.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425221903.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425221904.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 208, finish FWD profile ...
rank:208, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425221906.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221909.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221912.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425221915.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221917.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425221920.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221922.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425221925.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:208, finish BWD profile ...
rank:208,optimizer_step time: 4.877312183380127
rank:208, finish optimizer.step profile ...
rank:208, Before memory release - Allocated: 2994272256, Reserved: 3091202048
rank:208, trace log has been written to txt...
rank:208, finish release GPU memory ...
rank:208, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 209, finish warm up ...
rank_id = 209, input_tensor_shapes: [(512, 1, 4096)]
rank:209,cuda fwd time: 7.703551769256592
rank:209, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425222148.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222149.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222150.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222151.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222152.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222153.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222154.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222154.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 209, finish FWD profile ...
rank:209, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425222157.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222160.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425222162.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222165.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222167.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222170.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222172.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222175.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:209, finish BWD profile ...
rank:209,optimizer_step time: 4.928512096405029
rank:209, finish optimizer.step profile ...
rank:209, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:209, trace log has been written to txt...
rank:209, finish release GPU memory ...
rank:209, After memory release - Allocated: 910401024, Reserved: 1484783616
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 210, finish warm up ...
rank_id = 210, input_tensor_shapes: [(512, 1, 4096)]
rank:210,cuda fwd time: 7.5581440925598145
rank:210, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425222550.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222551.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222552.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222553.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222554.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222555.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222556.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222556.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 210, finish FWD profile ...
rank:210, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425222559.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222562.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222564.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222567.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425222569.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222572.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222574.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222577.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:210, finish BWD profile ...
rank:210,optimizer_step time: 5.350399971008301
rank:210, finish optimizer.step profile ...
rank:210, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:210, trace log has been written to txt...
rank:210, finish release GPU memory ...
rank:210, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 211, finish warm up ...
rank_id = 211, input_tensor_shapes: [(512, 1, 4096)]
rank:211,cuda fwd time: 7.717887878417969
rank:211, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425222823.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222824.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222825.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222825.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222827.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222827.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222828.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425222829.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 211, finish FWD profile ...
rank:211, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425222832.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222834.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425222837.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222840.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425222842.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222845.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222847.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425222850.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:211, finish BWD profile ...
rank:211,optimizer_step time: 4.942848205566406
rank:211, finish optimizer.step profile ...
rank:211, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:211, trace log has been written to txt...
rank:211, finish release GPU memory ...
rank:211, After memory release - Allocated: 910401024, Reserved: 1432354816
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 212, finish warm up ...
rank_id = 212, input_tensor_shapes: [(512, 1, 4096)]
rank:212,cuda fwd time: 8.764415740966797
rank:212, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425223100.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223101.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223102.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223103.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223104.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223105.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223106.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223107.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 212, finish FWD profile ...
rank:212, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425223109.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223112.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223115.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223117.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223120.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223122.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223125.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223127.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:212, finish BWD profile ...
rank:212,optimizer_step time: 4.9203200340271
rank:212, finish optimizer.step profile ...
rank:212, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:212, trace log has been written to txt...
rank:212, finish release GPU memory ...
rank:212, After memory release - Allocated: 911187456, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 213, finish warm up ...
rank_id = 213, input_tensor_shapes: [(512, 1, 4096)]
rank:213,cuda fwd time: 10.853376388549805
rank:213, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.03,timestamp=2425223619.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425223620.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425223622.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.03,timestamp=2425223623.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223624.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223625.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223627.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223628.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 213, finish FWD profile ...
rank:213, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425223630.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223633.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223636.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223639.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223641.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223644.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425223646.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223648.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:213, finish BWD profile ...
rank:213,optimizer_step time: 5.035007953643799
rank:213, finish optimizer.step profile ...
rank:213, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:213, trace log has been written to txt...
rank:213, finish release GPU memory ...
rank:213, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 214, finish warm up ...
rank_id = 214, input_tensor_shapes: [(512, 1, 4096)]
rank:214,cuda fwd time: 7.994368076324463
rank:214, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425223882.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223882.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223884.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223884.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223885.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223886.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223887.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425223888.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 214, finish FWD profile ...
rank:214, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425223891.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223894.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223896.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223899.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223901.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223904.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223906.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425223909.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:214, finish BWD profile ...
rank:214,optimizer_step time: 4.935679912567139
rank:214, finish optimizer.step profile ...
rank:214, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:214, trace log has been written to txt...
rank:214, finish release GPU memory ...
rank:214, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 215, finish warm up ...
rank_id = 215, input_tensor_shapes: [(512, 1, 4096)]
rank:215,cuda fwd time: 8.337408065795898
rank:215, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425224147.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224148.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224149.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224150.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224151.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224152.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224153.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224154.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 215, finish FWD profile ...
rank:215, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425224156.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.1,timestamp=2425224159.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224162.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224164.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224167.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224169.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224172.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224174.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:215, finish BWD profile ...
rank:215,optimizer_step time: 4.951039791107178
rank:215, finish optimizer.step profile ...
rank:215, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:215, trace log has been written to txt...
rank:215, finish release GPU memory ...
rank:215, After memory release - Allocated: 910401024, Reserved: 1486880768
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 216, finish warm up ...
rank_id = 216, input_tensor_shapes: [(512, 1, 4096)]
rank:216,cuda fwd time: 7.959551811218262
rank:216, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425224410.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224411.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224412.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224413.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224414.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224414.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224416.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224416.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 216, finish FWD profile ...
rank:216, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425224419.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224422.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224424.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224427.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224429.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224432.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224434.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224437.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:216, finish BWD profile ...
rank:216,optimizer_step time: 4.932608127593994
rank:216, finish optimizer.step profile ...
rank:216, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:216, trace log has been written to txt...
rank:216, finish release GPU memory ...
rank:216, After memory release - Allocated: 911187456, Reserved: 1444937728
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 217, finish warm up ...
rank_id = 217, input_tensor_shapes: [(512, 1, 4096)]
rank:217,cuda fwd time: 7.517183780670166
rank:217, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425224654.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224655.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224656.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224657.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224658.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224659.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224660.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224660.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 217, finish FWD profile ...
rank:217, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425224663.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224666.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224668.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224671.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224673.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224676.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425224678.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224681.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:217, finish BWD profile ...
rank:217,optimizer_step time: 4.9100799560546875
rank:217, finish optimizer.step profile ...
rank:217, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:217, trace log has been written to txt...
rank:217, finish release GPU memory ...
rank:217, After memory release - Allocated: 910401024, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 218, finish warm up ...
rank_id = 218, input_tensor_shapes: [(512, 1, 4096)]
rank:218,cuda fwd time: 7.300096035003662
rank:218, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425224909.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224909.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224910.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224911.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224912.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224913.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224914.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425224915.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 218, finish FWD profile ...
rank:218, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425224917.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224920.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224922.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224925.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224927.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224930.53,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224932.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425224935.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:218, finish BWD profile ...
rank:218,optimizer_step time: 4.915200233459473
rank:218, finish optimizer.step profile ...
rank:218, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:218, trace log has been written to txt...
rank:218, finish release GPU memory ...
rank:218, After memory release - Allocated: 911187456, Reserved: 1463812096
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 219, finish warm up ...
rank_id = 219, input_tensor_shapes: [(512, 1, 4096)]
rank:219,cuda fwd time: 7.341055870056152
rank:219, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425225145.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225146.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225147.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225147.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225149.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225149.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225150.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225151.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 219, finish FWD profile ...
rank:219, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425225153.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225156.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225159.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225161.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425225164.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225166.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225169.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225171.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:219, finish BWD profile ...
rank:219,optimizer_step time: 4.90393590927124
rank:219, finish optimizer.step profile ...
rank:219, Before memory release - Allocated: 2993059840, Reserved: 3116367872
rank:219, trace log has been written to txt...
rank:219, finish release GPU memory ...
rank:219, After memory release - Allocated: 910401024, Reserved: 1507852288
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 220, finish warm up ...
rank_id = 220, input_tensor_shapes: [(512, 1, 4096)]
rank:220,cuda fwd time: 8.850432395935059
rank:220, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425225375.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225375.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225377.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225377.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225378.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225380.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225381.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225382.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 220, finish FWD profile ...
rank:220, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425225385.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225388.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225390.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225393.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225395.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225398.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425225400.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225403.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:220, finish BWD profile ...
rank:220,optimizer_step time: 4.963327884674072
rank:220, finish optimizer.step profile ...
rank:220, Before memory release - Allocated: 2993584128, Reserved: 3072327680
rank:220, trace log has been written to txt...
rank:220, finish release GPU memory ...
rank:220, After memory release - Allocated: 911187456, Reserved: 1465909248
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 221, finish warm up ...
rank_id = 221, input_tensor_shapes: [(512, 1, 4096)]
rank:221,cuda fwd time: 7.29088020324707
rank:221, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425225624.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225625.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225626.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225627.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225628.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225629.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225630.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225630.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 221, finish FWD profile ...
rank:221, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425225633.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225636.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225638.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225641.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425225643.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225646.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225648.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225651.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:221, finish BWD profile ...
rank:221,optimizer_step time: 4.896768093109131
rank:221, finish optimizer.step profile ...
rank:221, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:221, trace log has been written to txt...
rank:221, finish release GPU memory ...
rank:221, After memory release - Allocated: 910401024, Reserved: 1421869056
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 222, finish warm up ...
rank_id = 222, input_tensor_shapes: [(512, 1, 4096)]
rank:222,cuda fwd time: 7.474175930023193
rank:222, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425225873.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225873.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225874.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225875.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225876.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225877.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225878.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425225879.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 222, finish FWD profile ...
rank:222, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425225881.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225884.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225886.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225889.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225891.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225894.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225896.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425225899.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:222, finish BWD profile ...
rank:222,optimizer_step time: 4.925439834594727
rank:222, finish optimizer.step profile ...
rank:222, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:222, trace log has been written to txt...
rank:222, finish release GPU memory ...
rank:222, After memory release - Allocated: 911187456, Reserved: 1442840576
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (101270272 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 223, finish warm up ...
rank_id = 223, input_tensor_shapes: [(512, 1, 4096)]
rank:223,cuda fwd time: 7.332863807678223
rank:223, fwd_subop num: 8, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425226107.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226108.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226109.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226110.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226111.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226112.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226113.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226113.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce']
rank_id = 223, finish FWD profile ...
rank:223, bwd_subop num: 8, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425226116.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226119.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226121.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226124.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226126.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226129.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425226131.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226134.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:223, finish BWD profile ...
rank:223,optimizer_step time: 4.890624046325684
rank:223, finish optimizer.step profile ...
rank:223, Before memory release - Allocated: 2993584128, Reserved: 3070230528
rank:223, trace log has been written to txt...
rank:223, finish release GPU memory ...
rank:223, After memory release - Allocated: 910401024, Reserved: 1484783616
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 224, finish warm up ...
rank_id = 224, input_tensor_shapes: [(512, 1, 4096)]
rank:224,cuda fwd time: 8.959615707397461
rank:224, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425226374.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226375.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226376.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226377.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226378.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226379.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226380.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226381.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425226381.69,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425226382.06,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425226382.21,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 224, finish FWD profile ...
rank:224, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425226383.62,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425226384.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226385.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425226386.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226387.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425226388.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226389.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425226390.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226391.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:224, finish BWD profile ...
rank:224,optimizer_step time: 5.604351997375488
rank:224, finish optimizer.step profile ...
rank:224, Before memory release - Allocated: 2858592768, Reserved: 3407872000
rank:224, trace log has been written to txt...
rank:224, finish release GPU memory ...
rank:224, After memory release - Allocated: 520277504, Reserved: 1262485504
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 225, finish warm up ...
rank_id = 225, input_tensor_shapes: [(512, 1, 4096)]
rank:225,cuda fwd time: 7.261184215545654
rank:225, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425226626.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226627.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226628.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226628.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226629.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226630.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226631.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226631.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425226632.26,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425226632.56,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425226632.68,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 225, finish FWD profile ...
rank:225, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425226634.93,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226637.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226640.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226643.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226645.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226648.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226650.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226653.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226655.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:225, finish BWD profile ...
rank:225,optimizer_step time: 5.395455837249756
rank:225, finish optimizer.step profile ...
rank:225, Before memory release - Allocated: 2921164800, Reserved: 3057647616
rank:225, trace log has been written to txt...
rank:225, finish release GPU memory ...
rank:225, After memory release - Allocated: 518607872, Reserved: 1298137088
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (2, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 226, finish warm up ...
rank_id = 226, input_tensor_shapes: [(512, 1, 4096)]
rank:226,cuda fwd time: 7.886847972869873
rank:226, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425226854.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226854.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226855.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226856.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226857.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226858.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425226859.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425226859.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425226860.44,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425226860.76,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425226860.9,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 226, finish FWD profile ...
rank:226, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425226863.2,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226865.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226868.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.09,timestamp=2425226871.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226874.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226876.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226879.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226881.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425226884.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:226, finish BWD profile ...
rank:226,optimizer_step time: 5.438464164733887
rank:226, finish optimizer.step profile ...
rank:226, Before memory release - Allocated: 2921592320, Reserved: 3053453312
rank:226, trace log has been written to txt...
rank:226, finish release GPU memory ...
rank:226, After memory release - Allocated: 519656448, Reserved: 1235222528
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (3, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 227, finish warm up ...
rank_id = 227, input_tensor_shapes: [(512, 1, 4096)]
rank:227,cuda fwd time: 7.209983825683594
rank:227, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425227070.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227071.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227072.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227072.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227073.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227074.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425227075.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227075.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425227076.36,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425227076.66,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425227076.79,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 227, finish FWD profile ...
rank:227, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425227079.02,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227081.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227084.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227086.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227089.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227091.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227094.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227096.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227099.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:227, finish BWD profile ...
rank:227,optimizer_step time: 1.950719952583313
rank:227, finish optimizer.step profile ...
rank:227, Before memory release - Allocated: 1968264704, Reserved: 2074083328
rank:227, trace log has been written to txt...
rank:227, finish release GPU memory ...
rank:227, After memory release - Allocated: 518607872, Reserved: 1235222528
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (4, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 228, finish warm up ...
rank_id = 228, input_tensor_shapes: [(512, 1, 4096)]
rank:228,cuda fwd time: 7.443456172943115
rank:228, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425227286.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227286.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425227287.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227288.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425227289.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227290.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425227290.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227291.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425227292.04,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425227292.35,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425227292.48,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 228, finish FWD profile ...
rank:228, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425227294.73,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227297.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227299.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227302.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227305.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227307.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227310.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227312.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227315.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:228, finish BWD profile ...
rank:228,optimizer_step time: 1.9568639993667603
rank:228, finish optimizer.step profile ...
rank:228, Before memory release - Allocated: 1968788992, Reserved: 2069889024
rank:228, trace log has been written to txt...
rank:228, finish release GPU memory ...
rank:228, After memory release - Allocated: 519656448, Reserved: 1212153856
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (5, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 229, finish warm up ...
rank_id = 229, input_tensor_shapes: [(512, 1, 4096)]
rank:229,cuda fwd time: 7.200767993927002
rank:229, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425227488.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227489.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227490.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227490.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227491.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227492.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227493.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227493.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425227494.18,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425227494.49,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425227494.62,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 229, finish FWD profile ...
rank:229, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425227496.88,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227499.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227502.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227504.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227507.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227509.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227512.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227514.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227517.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:229, finish BWD profile ...
rank:229,optimizer_step time: 1.955839991569519
rank:229, finish optimizer.step profile ...
rank:229, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:229, trace log has been written to txt...
rank:229, finish release GPU memory ...
rank:229, After memory release - Allocated: 518607872, Reserved: 1212153856
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (6, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 230, finish warm up ...
rank_id = 230, input_tensor_shapes: [(512, 1, 4096)]
rank:230,cuda fwd time: 7.096320152282715
rank:230, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425227690.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227691.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227691.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227692.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227693.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227694.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227694.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227695.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425227695.95,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425227696.25,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425227696.38,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 230, finish FWD profile ...
rank:230, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425227698.61,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227700.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227703.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227706.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227708.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227711.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227713.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227716.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227718.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:230, finish BWD profile ...
rank:230,optimizer_step time: 1.9824639558792114
rank:230, finish optimizer.step profile ...
rank:230, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:230, trace log has been written to txt...
rank:230, finish release GPU memory ...
rank:230, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
 > number of parameters on (tensor, pipeline) model parallel rank (7, 7): 117658368
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 231, finish warm up ...
rank_id = 231, input_tensor_shapes: [(512, 1, 4096)]
rank:231,cuda fwd time: 7.097343921661377
rank:231, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425227896.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227896.85,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227897.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227898.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227899.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227899.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227900.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425227901.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425227901.78,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425227902.08,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425227902.2,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 231, finish FWD profile ...
rank:231, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425227904.46,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227906.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227909.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227911.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227914.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227916.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425227919.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227922.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425227924.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:231, finish BWD profile ...
rank:231,optimizer_step time: 1.9322880506515503
rank:231, finish optimizer.step profile ...
rank:231, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:231, trace log has been written to txt...
rank:231, finish release GPU memory ...
rank:231, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 232, finish warm up ...
rank_id = 232, input_tensor_shapes: [(512, 1, 4096)]
rank:232,cuda fwd time: 8.770751953125
rank:232, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425228113.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228114.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228115.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228115.99,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228117.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228117.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228118.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228119.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425228120.18,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228120.55,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228120.7,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 232, finish FWD profile ...
rank:232, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425228122.08,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425228122.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228125.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425228126.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228127.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425228128.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228129.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425228130.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228131.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:232, finish BWD profile ...
rank:232,optimizer_step time: 5.536767959594727
rank:232, finish optimizer.step profile ...
rank:232, Before memory release - Allocated: 2922116608, Reserved: 2984247296
rank:232, trace log has been written to txt...
rank:232, finish release GPU memory ...
rank:232, After memory release - Allocated: 519656448, Reserved: 1254096896
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 233, finish warm up ...
rank_id = 233, input_tensor_shapes: [(512, 1, 4096)]
rank:233,cuda fwd time: 7.080959796905518
rank:233, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425228328.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228328.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228329.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228330.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228331.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228331.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228332.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228333.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425228333.6,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228333.9,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425228334.03,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 233, finish FWD profile ...
rank:233, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425228336.28,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228338.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228341.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228343.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228346.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228348.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228351.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228353.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228356.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:233, finish BWD profile ...
rank:233,optimizer_step time: 1.931264042854309
rank:233, finish optimizer.step profile ...
rank:233, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:233, trace log has been written to txt...
rank:233, finish release GPU memory ...
rank:233, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 234, finish warm up ...
rank_id = 234, input_tensor_shapes: [(512, 1, 4096)]
rank:234,cuda fwd time: 7.1690239906311035
rank:234, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425228533.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228534.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228535.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228536.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228537.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228537.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228538.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228539.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425228539.57,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228539.88,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425228540.01,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 234, finish FWD profile ...
rank:234, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425228542.27,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228544.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228547.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228549.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228552.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228554.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228557.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228559.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228562.58,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:234, finish BWD profile ...
rank:234,optimizer_step time: 1.977344036102295
rank:234, finish optimizer.step profile ...
rank:234, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:234, trace log has been written to txt...
rank:234, finish release GPU memory ...
rank:234, After memory release - Allocated: 519656448, Reserved: 1212153856
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 235, finish warm up ...
rank_id = 235, input_tensor_shapes: [(512, 1, 4096)]
rank:235,cuda fwd time: 7.7066240310668945
rank:235, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425228750.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228751.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228752.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228752.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228753.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228754.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228755.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425228755.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425228756.44,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228756.77,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425228756.9,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 235, finish FWD profile ...
rank:235, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425228759.22,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228761.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228764.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228766.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228769.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228771.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228774.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228776.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425228779.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:235, finish BWD profile ...
rank:235,optimizer_step time: 1.9783680438995361
rank:235, finish optimizer.step profile ...
rank:235, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:235, trace log has been written to txt...
rank:235, finish release GPU memory ...
rank:235, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 236, finish warm up ...
rank_id = 236, input_tensor_shapes: [(512, 1, 4096)]
rank:236,cuda fwd time: 7.963647842407227
rank:236, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425228973.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228974.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228975.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228976.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228977.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228978.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228979.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425228979.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425228980.24,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228980.56,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425228980.7,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 236, finish FWD profile ...
rank:236, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425228983.0,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228985.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228988.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228990.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228993.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228995.51,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425228998.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229000.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229003.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:236, finish BWD profile ...
rank:236,optimizer_step time: 1.9763200283050537
rank:236, finish optimizer.step profile ...
rank:236, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:236, trace log has been written to txt...
rank:236, finish release GPU memory ...
rank:236, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 237, finish warm up ...
rank_id = 237, input_tensor_shapes: [(512, 1, 4096)]
rank:237,cuda fwd time: 7.630847930908203
rank:237, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425229184.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229185.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229186.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229186.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229187.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229188.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229189.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229189.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425229190.39,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425229190.71,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425229190.84,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 237, finish FWD profile ...
rank:237, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425229193.12,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229195.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229198.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229200.64,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229203.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229205.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229208.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229210.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229213.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:237, finish BWD profile ...
rank:237,optimizer_step time: 1.9742720127105713
rank:237, finish optimizer.step profile ...
rank:237, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:237, trace log has been written to txt...
rank:237, finish release GPU memory ...
rank:237, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 238, finish warm up ...
rank_id = 238, input_tensor_shapes: [(512, 1, 4096)]
rank:238,cuda fwd time: 8.384511947631836
rank:238, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425229423.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229424.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229425.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229426.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229427.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229428.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229429.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229430.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425229430.55,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425229430.89,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425229431.04,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 238, finish FWD profile ...
rank:238, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425229433.4,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229435.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229438.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229440.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425229443.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229445.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229448.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229451.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229453.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:238, finish BWD profile ...
rank:238,optimizer_step time: 1.9875839948654175
rank:238, finish optimizer.step profile ...
rank:238, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:238, trace log has been written to txt...
rank:238, finish release GPU memory ...
rank:238, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 239, finish warm up ...
rank_id = 239, input_tensor_shapes: [(512, 1, 4096)]
rank:239,cuda fwd time: 7.207935810089111
rank:239, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425229644.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229644.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229645.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229646.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229647.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229647.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229648.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425229649.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425229649.75,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425229650.05,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425229650.18,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 239, finish FWD profile ...
rank:239, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425229652.47,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229654.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229657.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229659.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229662.68,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229664.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.07,timestamp=2425229667.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229669.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425229672.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:239, finish BWD profile ...
rank:239,optimizer_step time: 1.9711999893188477
rank:239, finish optimizer.step profile ...
rank:239, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:239, trace log has been written to txt...
rank:239, finish release GPU memory ...
rank:239, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 240, finish warm up ...
rank_id = 240, input_tensor_shapes: [(512, 1, 4096)]
rank:240,cuda fwd time: 8.718144416809082
rank:240, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425229850.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229851.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229852.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229853.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229854.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229855.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229856.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229857.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425229857.66,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425229858.03,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425229858.19,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 240, finish FWD profile ...
rank:240, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425229859.53,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425229860.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229861.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425229862.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229863.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425229863.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229864.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425229865.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425229866.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:240, finish BWD profile ...
rank:240,optimizer_step time: 5.584896087646484
rank:240, finish optimizer.step profile ...
rank:240, Before memory release - Allocated: 2921592320, Reserved: 3051356160
rank:240, trace log has been written to txt...
rank:240, finish release GPU memory ...
rank:240, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 241, finish warm up ...
rank_id = 241, input_tensor_shapes: [(512, 1, 4096)]
rank:241,cuda fwd time: 7.250944137573242
rank:241, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425230060.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230061.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230062.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230063.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230064.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230064.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230065.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230066.13,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425230066.61,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425230066.92,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425230067.04,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 241, finish FWD profile ...
rank:241, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425230069.31,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.08,timestamp=2425230071.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230074.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230076.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230078.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230080.78,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230083.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230085.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230087.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:241, finish BWD profile ...
rank:241,optimizer_step time: 1.97324800491333
rank:241, finish optimizer.step profile ...
rank:241, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:241, trace log has been written to txt...
rank:241, finish release GPU memory ...
rank:241, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 242, finish warm up ...
rank_id = 242, input_tensor_shapes: [(512, 1, 4096)]
rank:242,cuda fwd time: 7.252992153167725
rank:242, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425230265.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230266.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230266.94,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230267.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230268.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230269.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230269.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230270.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425230271.07,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425230271.38,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425230271.51,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 242, finish FWD profile ...
rank:242, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425230273.76,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230276.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230278.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230281.31,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230284.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230286.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230289.11,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230291.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230294.04,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:242, finish BWD profile ...
rank:242,optimizer_step time: 1.97324800491333
rank:242, finish optimizer.step profile ...
rank:242, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:242, trace log has been written to txt...
rank:242, finish release GPU memory ...
rank:242, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 243, finish warm up ...
rank_id = 243, input_tensor_shapes: [(512, 1, 4096)]
rank:243,cuda fwd time: 7.923711776733398
rank:243, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425230476.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230476.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230477.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230478.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230479.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230480.25,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230481.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230481.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425230482.4,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425230482.74,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425230482.87,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 243, finish FWD profile ...
rank:243, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425230485.19,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230487.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230490.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230492.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230495.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230497.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230500.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230502.76,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230505.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:243, finish BWD profile ...
rank:243,optimizer_step time: 1.9937280416488647
rank:243, finish optimizer.step profile ...
rank:243, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:243, trace log has been written to txt...
rank:243, finish release GPU memory ...
rank:243, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 244, finish warm up ...
rank_id = 244, input_tensor_shapes: [(512, 1, 4096)]
rank:244,cuda fwd time: 8.10700798034668
rank:244, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425230705.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230706.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230707.21,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230707.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230708.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230709.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230710.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230711.28,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425230711.8,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425230712.15,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425230712.29,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 244, finish FWD profile ...
rank:244, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425230714.61,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230716.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230719.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230722.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230724.87,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230727.12,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230729.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230732.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230734.82,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:244, finish BWD profile ...
rank:244,optimizer_step time: 1.9875839948654175
rank:244, finish optimizer.step profile ...
rank:244, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:244, trace log has been written to txt...
rank:244, finish release GPU memory ...
rank:244, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 245, finish warm up ...
rank_id = 245, input_tensor_shapes: [(512, 1, 4096)]
rank:245,cuda fwd time: 7.833600044250488
rank:245, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425230920.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230921.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230922.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230923.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230924.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230924.74,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425230925.69,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425230926.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425230926.85,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425230927.19,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425230927.32,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 245, finish FWD profile ...
rank:245, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425230929.64,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230932.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230934.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230937.18,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230939.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425230942.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230944.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230947.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425230949.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:245, finish BWD profile ...
rank:245,optimizer_step time: 1.977344036102295
rank:245, finish optimizer.step profile ...
rank:245, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:245, trace log has been written to txt...
rank:245, finish release GPU memory ...
rank:245, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 246, finish warm up ...
rank_id = 246, input_tensor_shapes: [(512, 1, 4096)]
rank:246,cuda fwd time: 7.271423816680908
rank:246, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425231127.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231128.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231129.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231129.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231130.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231131.52,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231132.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231133.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425231133.49,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425231133.8,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425231133.93,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 246, finish FWD profile ...
rank:246, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425231136.2,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231138.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231141.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231143.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231146.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231148.84,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231151.55,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425231153.8,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231156.47,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:246, finish BWD profile ...
rank:246,optimizer_step time: 1.985535979270935
rank:246, finish optimizer.step profile ...
rank:246, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:246, trace log has been written to txt...
rank:246, finish release GPU memory ...
rank:246, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 247, finish warm up ...
rank_id = 247, input_tensor_shapes: [(512, 1, 4096)]
rank:247,cuda fwd time: 7.412735939025879
rank:247, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425231332.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231333.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231334.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231334.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231335.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231336.5,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231337.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231338.03,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425231338.51,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425231338.83,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425231338.96,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 247, finish FWD profile ...
rank:247, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425231341.22,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231343.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231346.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425231348.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231351.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231353.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231356.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231358.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231361.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:247, finish BWD profile ...
rank:247,optimizer_step time: 1.985535979270935
rank:247, finish optimizer.step profile ...
rank:247, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:247, trace log has been written to txt...
rank:247, finish release GPU memory ...
rank:247, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 248, finish warm up ...
rank_id = 248, input_tensor_shapes: [(512, 1, 4096)]
rank:248,cuda fwd time: 9.087648391723633
rank:248, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425231540.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231541.24,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231542.39,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231543.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231544.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231545.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231546.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231546.9,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425231547.45,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425231547.84,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425231547.99,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 248, finish FWD profile ...
rank:248, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.0,timestamp=2425231549.36,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425231550.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231551.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425231552.15,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231553.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425231553.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231554.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.0,timestamp=2425231555.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231556.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:248, finish BWD profile ...
rank:248,optimizer_step time: 5.582848072052002
rank:248, finish optimizer.step profile ...
rank:248, Before memory release - Allocated: 2921592320, Reserved: 3051356160
rank:248, trace log has been written to txt...
rank:248, finish release GPU memory ...
rank:248, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 249, finish warm up ...
rank_id = 249, input_tensor_shapes: [(512, 1, 4096)]
rank:249,cuda fwd time: 7.264256000518799
rank:249, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425231790.67,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231791.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231792.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231792.89,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231793.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231794.4,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425231795.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425231795.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425231796.4,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425231796.7,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425231796.83,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 249, finish FWD profile ...
rank:249, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425231799.06,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231801.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231804.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231806.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231809.34,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425231811.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231814.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425231816.62,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425231819.32,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:249, finish BWD profile ...
rank:249,optimizer_step time: 1.9793920516967773
rank:249, finish optimizer.step profile ...
rank:249, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:249, trace log has been written to txt...
rank:249, finish release GPU memory ...
rank:249, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 250, finish warm up ...
rank_id = 250, input_tensor_shapes: [(512, 1, 4096)]
rank:250,cuda fwd time: 7.288832187652588
rank:250, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425232030.35,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232031.01,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232031.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232032.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232033.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232034.05,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232034.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232035.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425232036.08,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425232036.39,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425232036.52,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 250, finish FWD profile ...
rank:250, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425232038.78,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232041.16,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232044.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232046.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232049.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232051.38,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232054.1,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425232056.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232059.02,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:250, finish BWD profile ...
rank:250,optimizer_step time: 1.9834879636764526
rank:250, finish optimizer.step profile ...
rank:250, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:250, trace log has been written to txt...
rank:250, finish release GPU memory ...
rank:250, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 251, finish warm up ...
rank_id = 251, input_tensor_shapes: [(512, 1, 4096)]
rank:251,cuda fwd time: 7.179264068603516
rank:251, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425232237.42,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232238.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232238.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232239.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232240.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232241.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232241.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232242.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425232243.06,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425232243.36,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425232243.49,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 251, finish FWD profile ...
rank:251, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425232245.72,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232248.08,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232250.92,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232253.23,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232255.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232258.19,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232260.91,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232263.22,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232265.93,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:251, finish BWD profile ...
rank:251,optimizer_step time: 1.9804160594940186
rank:251, finish optimizer.step profile ...
rank:251, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:251, trace log has been written to txt...
rank:251, finish release GPU memory ...
rank:251, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 252, finish warm up ...
rank_id = 252, input_tensor_shapes: [(512, 1, 4096)]
rank:252,cuda fwd time: 7.945216178894043
rank:252, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425232621.09,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232621.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232622.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232623.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232624.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232625.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232626.14,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232626.81,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425232627.31,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425232627.65,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425232627.79,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 252, finish FWD profile ...
rank:252, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425232630.09,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232632.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232635.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232637.63,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232640.36,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425232642.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232645.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232647.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232650.27,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:252, finish BWD profile ...
rank:252,optimizer_step time: 1.9834879636764526
rank:252, finish optimizer.step profile ...
rank:252, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:252, trace log has been written to txt...
rank:252, finish release GPU memory ...
rank:252, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 253, finish warm up ...
rank_id = 253, input_tensor_shapes: [(512, 1, 4096)]
rank:253,cuda fwd time: 7.907328128814697
rank:253, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425232849.26,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232849.98,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232850.97,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232851.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232852.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232853.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425232854.3,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425232854.96,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425232855.47,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425232855.79,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425232855.93,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 253, finish FWD profile ...
rank:253, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425232858.22,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232860.6,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232863.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232865.77,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232868.48,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232870.73,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232873.45,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232875.79,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425232878.49,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:253, finish BWD profile ...
rank:253,optimizer_step time: 1.9875839948654175
rank:253, finish optimizer.step profile ...
rank:253, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:253, trace log has been written to txt...
rank:253, finish release GPU memory ...
rank:253, After memory release - Allocated: 518607872, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 254, finish warm up ...
rank_id = 254, input_tensor_shapes: [(512, 1, 4096)]
rank:254,cuda fwd time: 7.747583866119385
rank:254, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.02,timestamp=2425233062.37,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233063.07,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425233064.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233064.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425233065.66,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425233066.33,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.02,timestamp=2425233067.29,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233067.95,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425233068.44,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425233068.77,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425233068.91,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 254, finish FWD profile ...
rank:254, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.06,timestamp=2425233071.19,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233073.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233076.41,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233078.72,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233081.44,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233083.71,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233086.43,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233088.75,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233091.46,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:254, finish BWD profile ...
rank:254,optimizer_step time: 1.977344036102295
rank:254, finish optimizer.step profile ...
rank:254, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:254, trace log has been written to txt...
rank:254, finish release GPU memory ...
rank:254, After memory release - Allocated: 519656448, Reserved: 1233125376
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      6
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:mock = False
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.949), (0.949, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
WARNING:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(6, 0, 0), and config=GPTDatasetConfig(random_seed=1403, sequence_length=512, blend=(['/research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document'], None), blend_per_split=[None, None, None], split='949,50,1', split_matrix=[(0, 0.949), (0.949, 0.999), (0.999, 1.0)], path_to_cache='./data_cache/llama2_7_pretrain_WS8_TP2_PP2', mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._Llama2Tokenizer object at 0x7fb9571727f0>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /research/d1/gds/ytyang/yichengfeng/Megatron-LM/data/output_prefix_llama/output_prefix_llama_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 79000
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 79000
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 23139314544af8f169c4daa4662c9d41-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 495054
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 450f97cb1fe16d623e863d2873d34f64-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 26327
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 226d7878146acf063dab805930021f14-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 226d7878146acf063dab805930021f14-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 226d7878146acf063dab805930021f14-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 846
> finished creating GPT datasets ...
building GPT model ...
args.is_scaling_mode:True
use mcore models, use_te = True
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with DistributedDataParallelConfig: DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, use_distributed_optimizer=False, check_for_nan_in_grad=False, bucket_size=None)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.param_and_grad_buffer:Params for bucket 1 (117658368 elements):
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_qkv.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc2.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.final_layernorm.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc2.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.output_layer.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.mlp.linear_fc1.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.2.self_attention.linear_proj.bias
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
INFO:megatron.core.distributed.param_and_grad_buffer:    module.decoder.layers.0.self_attention.linear_proj.bias
INFO:megatron.core.optimizer:Setting up optimizer with OptimizerConfig: OptimizerConfig(optimizer='adam', lr=3e-05, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=True, bf16=False, params_dtype=torch.float16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fb9571725e0>)
> learning rate decay style: cosine
rank_id = 255, finish warm up ...
rank_id = 255, input_tensor_shapes: [(512, 1, 4096)]
rank:255,cuda fwd time: 7.214079856872559
rank:255, fwd_subop num: 11, fwd_subop: ['trace_src_func=allreduce,duration=0.01,timestamp=2425233268.0,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233268.65,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233269.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233270.17,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233271.06,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233271.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233272.59,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.01,timestamp=2425233273.2,input__shape=[512, 4096],input__dtype=torch.float16,func_name=linear_fwd,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.0,timestamp=2425233273.67,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_1,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.02,timestamp=2425233273.97,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_2,group=tp,comm_func=allreduce', 'trace_src_func=_reduce,duration=0.01,timestamp=2425233274.1,input__shape=[512, 1],input__dtype=torch.float32,func_name=crossEntropy_fwd_3,group=tp,comm_func=allreduce']
rank_id = 255, finish FWD profile ...
rank:255, bwd_subop num: 9, bwd_subop: ['trace_src_func=allreduce,duration=0.07,timestamp=2425233276.36,input__shape=[512, 1, 4096],input__dtype=torch.float16,func_name=embedding_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233278.7,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233281.54,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233283.86,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233286.56,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.05,timestamp=2425233288.83,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233291.57,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233293.88,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce', 'trace_src_func=allreduce,duration=0.06,timestamp=2425233296.61,input__shape=[512, 4096],input__dtype=torch.float16,func_name=normlinear_bwd,group=tp,comm_func=allreduce']
rank:255, finish BWD profile ...
rank:255,optimizer_step time: 1.985535979270935
rank:255, finish optimizer.step profile ...
rank:255, Before memory release - Allocated: 1968264704, Reserved: 2071986176
rank:255, trace log has been written to txt...
rank:255, finish release GPU memory ...
rank:255, After memory release - Allocated: 518607872, Reserved: 1233125376
